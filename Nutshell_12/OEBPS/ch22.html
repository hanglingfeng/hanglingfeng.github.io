<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Parallel Programming</title><link href="epub.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:3330d66d-9080-4595-aa6c-b8113bd76e5a" name="Adept.expected.resource"/></head><body data-type="book"><section data-nutshell-tab="Parallel Programming" data-pdf-bookmark="Chapter 22. Parallel Programming" data-type="chapter" epub:type="chapter"><div class="chapter" id="parallel_programming-id00071">
<h1><span class="label">Chapter 22. </span>Parallel Programming</h1>
<p><a contenteditable="false" data-primary="parallel programming" data-type="indexterm" id="ch22.html100"/>In this chapter, we cover the multithreading APIs and constructs aimed at leveraging multicore processors:</p>
<ul>
<li><p>Parallel LINQ or <em>PLINQ</em></p></li>
<li><p>The <code>Parallel</code> class</p></li>
<li><p>The <em>task parallelism</em> constructs</p></li>
<li><p>The <em>concurrent collections</em></p></li>
</ul>
<p><a contenteditable="false" data-primary="Parallel Framework (PFX)" data-type="indexterm" id="ch22.html101"/>These constructs are collectively known (loosely) as Parallel Framework (PFX). The <code>Parallel</code> class together with the task parallelism constructs is called the <em>Task Parallel Library</em> (TPL).</p>
<p>You’ll need to be comfortable with the fundamentals in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a> before reading this chapter—particularly locking, thread safety, and the <code>Task</code> class.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>.NET offers a number of additional specialized APIs to help with parallel and asynchronous programming:</p>
<ul>
<li><p><a contenteditable="false" data-primary="System..." data-secondary="System.Threading.Channels.Channel" data-type="indexterm" id="id4449"/><code>System.Threading.Channels.Channel</code> is a high-performance asynchronous producer/consumer queue, introduced in .NET Core 3.</p></li>
<li><p><a contenteditable="false" data-primary="Microsoft Dataflow" data-type="indexterm" id="id4450"/><em>Microsoft Dataflow</em> (in the <code>System.Thread⁠ing.Tasks​.Dataflow</code> namespace) is a sophisticated API for creating networks of buffered <em>blocks</em> that execute actions or data transformations in parallel, with a semblance to actor/agent programming.</p></li>
<li><p><a contenteditable="false" data-primary="Reactive Extensions" data-type="indexterm" id="id4451"/><em>Reactive Extensions</em> implements LINQ over <code>IObservable</code> (an alternative abstraction to <code>IAsyncEnumerable</code>) and excels at combining asynchronous streams. Reactive extensions ships in the <em>System.Reactive</em> NuGet package.</p></li>
</ul>
</div>
<section data-pdf-bookmark="Why PFX?" data-type="sect1"><div class="sect1" id="why_pfxquestion_mark">
<h1>Why PFX?</h1>
<p><a contenteditable="false" data-primary="Parallel Framework (PFX)" data-secondary="benefits of" data-type="indexterm" id="ch22.html102"/><a contenteditable="false" data-primary="parallel programming" data-secondary="PFX benefits" data-type="indexterm" id="ch22.html103"/>Over the past 15 years, CPU manufacturers have shifted from single-core to multicore processors. This is problematic for us as programmers because single-threaded code does not automatically run faster as a result of those extra cores.</p>
<p>Utilizing multiple cores is easy for most server applications, where each thread can independently handle a separate client request, but it’s more difficult on the desktop because it typically requires that you take your computationally intensive code and do the following:</p>
<ol>
<li><p><em>Partition</em> it into small chunks.</p></li>
<li><p>Execute those chunks in parallel via multithreading.</p></li>
<li><p><em>Collate</em> the results as they become available, in a thread-safe and performant manner.</p></li>
</ol>
<p>Although you can do all of this with the classic multithreading constructs, it’s awkward—particularly the steps of partitioning and collating. A further problem is that the usual strategy of locking for thread safety causes a lot of contention when many threads work on the same data at once.</p>
<p>The PFX libraries have been designed specifically to help in these scenarios.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Programming to leverage multicores or multiple processors is called <em>parallel programming</em>. This is a subset of the broader concept of multithreading.</p>
</div>
<section data-pdf-bookmark="PFX Concepts" data-type="sect2"><div class="sect2" id="pfx_concepts">
<h2>PFX Concepts</h2>
<p><a contenteditable="false" data-primary="Parallel Framework (PFX)" data-secondary="concepts" data-type="indexterm" id="id4452"/>There are two strategies for partitioning work among threads: <em>data parallelism</em> and <em>task parallelism</em>.</p>
<p><a contenteditable="false" data-primary="data parallelism" data-type="indexterm" id="id4453"/>When a set of tasks must be performed on many data values, we can parallelize by having each thread perform the (same) set of tasks on a subset of values. This is called <em>data parallelism</em> because we are partitioning the <em>data</em> between threads. <a contenteditable="false" data-primary="task parallelism" data-secondary="defined" data-type="indexterm" id="id4454"/>In contrast, with <em>task parallelism</em> we partition the <em>tasks</em>; in other words, we have each thread perform a different task.</p>
<p>In general, data parallelism is easier and scales better to highly parallel hardware because it reduces or eliminates shared data (thereby reducing contention and thread-safety issues). Also, data parallelism exploits the fact that there are often more data values than discrete tasks, increasing the parallelism potential.</p>
<p><a contenteditable="false" data-primary="structured parallelism" data-type="indexterm" id="id4455"/>Data parallelism is also conducive to <em>structured parallelism</em>, which means that parallel work units start and finish in the same place in your program. In contrast, task parallelism tends to be unstructured, meaning that parallel work units may start and finish in places scattered across your program. Structured parallelism is simpler and less error prone and allows you to farm the difficult job of partitioning and thread coordination (and even result collation) out to libraries.</p>
</div></section>
<section data-pdf-bookmark="PFX Components" data-type="sect2"><div class="sect2" id="pfx_components-id00034">
<h2>PFX Components</h2>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="functionality" data-type="indexterm" id="id4456"/><a contenteditable="false" data-primary="Parallel Framework (PFX)" data-secondary="components" data-type="indexterm" id="ch22.html104"/>PFX comprises two layers of functionality, as shown in <a data-type="xref" href="#pfx_components-id00085">Figure 22-1</a>. The higher layer consists of two <em>structured data parallelism</em> APIs: PLINQ and the <code>Parallel</code> class. The lower layer contains the task parallelism classes—plus a set of additional constructs to help with parallel programming activities.</p>
<figure><div class="figure" id="pfx_components-id00085">
<img alt="PFX components" src="assets/cn10_2201.png"/>
<h6><span class="label">Figure 22-1. </span>PFX components</h6>
</div></figure>
<p>PLINQ offers the richest functionality: it automates all the steps of parallelization—including partitioning the work into tasks, executing those tasks on threads, and collating the results into a single output sequence. <a contenteditable="false" data-primary="declarative parallelism" data-type="indexterm" id="id4457"/>It’s called <em>declarative</em>—because you simply declare that you want to parallelize your work (which you structure as a LINQ query) and let the runtime take care of the implementation details. <a contenteditable="false" data-primary="imperative parallelism" data-type="indexterm" id="id4458"/>In contrast, the other approaches are <em>imperative</em>, in that you need to explicitly write code to partition or collate. As the following synopsis shows, in the case of the <code>Parallel</code> class, you must collate results yourself; with the task parallelism constructs, you must partition the work yourself, too:</p>
<table class="border">
<thead>
<tr>
<th/>
<th>Partitions work</th>
<th>Collates results</th>
</tr>
</thead>
<tbody>
<tr>
<td>PLINQ</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>The <code>Parallel</code> class</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>PFX’s task parallelism</td>
<td>No</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>The concurrent collections and spinning primitives help you with lower-level parallel programming activities. These are important because PFX has been designed to work not only with today’s hardware, but also with future generations of processors with far more cores. If you want to move a pile of chopped wood and you have 32 workers to do the job, the biggest challenge is moving the wood without the workers getting in each other’s way. It’s the same with dividing an algorithm among 32 cores: if ordinary locks are used to protect common resources, the resultant blocking can mean that only a fraction of those cores are ever actually busy at once. The concurrent collections are tuned specifically for highly concurrent access, with the focus on minimizing or eliminating blocking. PLINQ and the <code>Parallel</code> class themselves rely on the concurrent collections and on spinning primitives for efficient management of work.<a contenteditable="false" data-primary="" data-startref="ch22.html104" data-type="indexterm" id="id4459"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="other_uses_for_pfx">
<h1>Other Uses for PFX</h1>
<p>The parallel programming constructs are useful not only for leveraging multicores, but also in other scenarios:</p>
<ul>
<li><p>The concurrent collections are sometimes appropriate when you want a thread-safe queue, stack, or dictionary.</p></li>
<li><p><code>BlockingCollection</code> provides an easy means to implement producer/consumer structures, and is a good way to <em>limit</em> concurrency.</p></li>
<li><p>Tasks are the basis of asynchronous programming, as we saw in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a>.</p></li>
</ul>
</div></aside>
</div></section>
<section data-pdf-bookmark="When to Use PFX" data-type="sect2"><div class="sect2" id="when_to_use_pfx">
<h2>When to Use PFX</h2>
<p><a contenteditable="false" data-primary="Parallel Framework (PFX)" data-secondary="when to use" data-type="indexterm" id="id4460"/>The primary use case for PFX is <em>parallel programming</em>: leveraging multicore processors to speed up computationally intensive code.</p>
<p><a contenteditable="false" data-primary="Amdahl's law" data-type="indexterm" id="id4461"/>A challenge in parallel programming is Amdahl’s law, which states that the maximum performance improvement from parallelization is governed by the portion of the code that must execute sequentially. For instance, if only two-thirds of an algorithm’s execution time is parallelizable, you can never exceed a threefold performance gain—even with an infinite number of cores.</p>
<p>So, before proceeding, it’s worth verifying that the bottleneck is in parallelizable code. It’s also worth considering whether your code <em>needs</em> to be computationally intensive—optimization is often the easiest and most effective approach. There’s a trade-off, though, in that some optimization techniques can make it more difficult to parallelize code.</p>
<p><a contenteditable="false" data-primary="embarrassingly parallel problems" data-type="indexterm" id="id4462"/>The easiest gains come with what’s called <em>embarrassingly parallel</em> problems—this is when a job can be easily divided into tasks that efficiently execute on their own (structured parallelism is very well suited to such problems). Examples include many image-processing tasks, ray tracing, and brute-force approaches in mathematics or cryptography. An example of a non-embarrassingly parallel problem is implementing an optimized version of the quicksort algorithm—a good result takes some thought and might require unstructured parallelism.<a contenteditable="false" data-primary="" data-startref="ch22.html103" data-type="indexterm" id="id4463"/><a contenteditable="false" data-primary="" data-startref="ch22.html102" data-type="indexterm" id="id4464"/><a contenteditable="false" data-primary="" data-startref="ch22.html101" data-type="indexterm" id="id4465"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="PLINQ" data-type="sect1"><div class="sect1" id="plinq">
<h1>PLINQ</h1>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-type="indexterm" id="ch22.html105"/>PLINQ automatically parallelizes local LINQ queries. PLINQ has the advantage of being easy to use in that it offloads the burden of both work partitioning and result collation to .NET.</p>
<p><a contenteditable="false" data-primary="AsParallel operator" data-type="indexterm" id="ch22.html106"/>To use PLINQ, simply call <code>AsParallel()</code> on the input sequence and then continue the LINQ query as usual. The following query calculates the prime numbers between 3 and 100,000, making full use of all cores on the target machine:</p>
<pre data-type="programlisting">// Calculate prime numbers using a simple (unoptimized) algorithm.

IEnumerable&lt;int&gt; numbers = Enumerable.Range (3, 100000-3);

var parallelQuery = 
  from n in numbers<strong>.AsParallel()</strong>
  where Enumerable.Range (2, (int) Math.Sqrt (n)).All (i =&gt; n % i &gt; 0)
  select n;

int[] primes = parallelQuery.ToArray();</pre>
<p><code>AsParallel</code> is an extension method in <code>System.Linq.ParallelEnumerable</code>. It wraps the input in a sequence based on <code>ParallelQuery&lt;TSource&gt;</code>, which causes the LINQ query operators that you subsequently call to bind to an alternate set of extension methods defined in <code>ParallelEnumerable</code>. These provide parallel implementations of each of the standard query operators. Essentially, they work by partitioning the input sequence into chunks that execute on different threads, collating the results back into a single output sequence for consumption, as depicted in <a data-type="xref" href="#plinq_execution_model">Figure 22-2</a>.</p>
<figure><div class="figure" id="plinq_execution_model">
<img alt="PLINQ execution model" src="assets/cn10_2202.png"/>
<h6><span class="label">Figure 22-2. </span>PLINQ execution model</h6>
</div></figure>
<p>Calling <code>AsSequential()</code> unwraps a <code>ParallelQuery</code> sequence so that subsequent query operators bind to the standard query operators and execute sequentially. This is necessary before calling methods that have side effects or are not thread-safe.</p>
<p>For query operators that accept two input sequences (<code>Join</code>, <code>GroupJoin</code>, <code>Concat</code>, <code>Union</code>, <code>Intersect</code>, <code>Except</code>, and <code>Zip</code>), you must apply <code>AsParallel()</code> to both input sequences (otherwise, an exception is thrown). You don’t, however, need to keep applying <code>AsParallel</code> to a query as it progresses, because PLINQ’s query operators output another <code>ParallelQuery</code> sequence. In fact, calling <code>AsParallel</code> again introduces inefficiency in that it forces merging and repartitioning of the query:</p>
<pre data-type="programlisting">mySequence.AsParallel()           // Wraps sequence in ParallelQuery&lt;int&gt;
          .Where (n =&gt; n &gt; 100)   // Outputs another ParallelQuery&lt;int&gt;
    <strong>      .AsParallel()           // Unnecessary - and inefficient!</strong>
          .Select (n =&gt; n * n)</pre>
<p>Not all query operators can be effectively parallelized. For those that cannot (see <a data-type="xref" href="#plinq_limitations">“PLINQ Limitations”</a>), PLINQ implements the operator sequentially, instead. PLINQ might also operate sequentially if it suspects that the overhead of parallelization will actually slow a particular query.</p>
<p>PLINQ is only for local collections: it doesn’t work with Entity Framework, for instance, because in those cases the LINQ translates into SQL, which then executes on a database server. However, you <em>can</em> use PLINQ to perform additional local querying on the result sets obtained from database queries.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If a PLINQ query throws an exception, it’s rethrown as an <code>AggregateException</code> whose <code>InnerExceptions</code> property contains the real exception (or exceptions). For more details, see <a data-type="xref" href="#working_with_aggregateexception">“Working with AggregateException”</a>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="why_isnapostrophet_asparallel_the_defau">
<h1>Why Isn’t AsParallel the Default?</h1>
<p>Given that <code>AsParallel</code> transparently parallelizes LINQ queries, the question arises: Why didn’t Microsoft simply parallelize the standard query operators and make PLINQ the default?</p>
<p>There are a number of reasons for the <em>opt-in</em> approach. First, for PLINQ to be useful, there must be a reasonable amount of computationally intensive work for it to farm out to worker threads. Most LINQ-to-Objects queries execute very quickly; thus, not only would parallelization be unnecessary, but the overhead of partitioning, collating, and coordinating the extra threads might actually slow things down.</p>
<p>Additionally:</p>
<ul>
<li><p>The output of a PLINQ query (by default) can differ from a LINQ query with respect to element ordering (see <a data-type="xref" href="#plinq_and_ordering">“PLINQ and Ordering”</a>).</p></li>
<li><p>PLINQ wraps exceptions in an <code>AggregateException</code> (to handle the possibility of multiple exceptions being thrown).</p></li>
<li><p>PLINQ will give unreliable results if the query invokes thread-unsafe methods.</p></li>
</ul>
<p>Finally, PLINQ offers quite a few hooks for tuning and tweaking. Burdening the standard LINQ-to-Objects API with such nuances would add distraction.</p>
</div></aside>
<section data-pdf-bookmark="Parallel Execution Ballistics" data-type="sect2"><div class="sect2" id="parallel_execution_ballistics">
<h2>Parallel Execution Ballistics</h2>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="parallel execution ballistics" data-type="indexterm" id="id4466"/>Like ordinary LINQ queries, PLINQ queries are lazily evaluated. This means that execution is triggered only when you begin consuming the results—typically via a <code>foreach</code> loop (although it can also be via a conversion operator such as <code>ToArray</code> or an operator that returns a single element or value).</p>
<p>As you enumerate the results, though, execution proceeds somewhat differently from that of an ordinary sequential query. A sequential query is powered entirely by the consumer in a “pull” fashion: each element from the input sequence is fetched exactly when required by the consumer. A parallel query ordinarily uses independent threads to fetch elements from the input sequence slightly <em>ahead</em> of when they’re needed by the consumer (rather like a teleprompter for newsreaders). It then processes the elements in parallel through the query chain, holding the results in a small buffer so that they’re ready for the consumer on demand. If the consumer pauses or breaks out of the enumeration early, the query processor also pauses or stops so as not to waste CPU time or memory.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a contenteditable="false" data-primary="WithMergeOptions" data-type="indexterm" id="id4467"/>You can tweak PLINQ’s buffering behavior by calling <code>WithMergeOptions</code> after <code>AsParallel</code>. The default value of <code>AutoBuffered</code> generally gives the best overall results. <code>NotBuffered</code> disables the buffer and is useful if you want to see results as soon as possible; <code>FullyBuffered</code> caches the entire result set before presenting it to the consumer (the <code>OrderBy</code> and <code>Reverse</code> operators naturally work this way, as do the element, aggregation, and conversion operators).</p>
</div>
</div></section>
<section data-pdf-bookmark="PLINQ and Ordering" data-type="sect2"><div class="sect2" id="plinq_and_ordering">
<h2>PLINQ and Ordering</h2>
<p><a contenteditable="false" data-primary="ordering" data-secondary="PLINQ and" data-type="indexterm" id="id4468"/><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="ordering" data-type="indexterm" id="id4469"/>A side effect of parallelizing the query operators is that when the results are collated, it’s not necessarily in the same order that they were submitted (see <a data-type="xref" href="#plinq_execution_model">Figure 22-2</a>). In other words, LINQ’s normal order-preservation guarantee for sequences no longer holds.</p>
<p>If you need order preservation, you can force it by calling <code>AsOrdered()</code> after <code>AsParallel()</code>:</p>
<pre data-type="programlisting">myCollection.AsParallel().AsOrdered()...</pre>
<p>Calling <code>AsOrdered</code> incurs a performance hit with large numbers of elements because PLINQ must keep track of each element’s original position.</p>
<p>You can negate the effect of <code>AsOrdered</code> later in a query by calling <code>AsUnordered</code>: this introduces a “random shuffle point,” which allows the query to execute more efficiently from that point on. So, if you wanted to preserve input-sequence ordering for just the first two query operators, you’d do this:</p>
<pre data-type="programlisting">inputSequence.AsParallel()<strong>.AsOrdered()</strong>
  <em>.QueryOperator1</em>()
 <em> .QueryOperator2</em>()
  <strong>.AsUnordered()</strong>       // From here on, ordering doesn’t matter
  .<em>QueryOperator3</em>()
  ...</pre>
<p><code>AsOrdered</code> is not the default because for most queries, the original input ordering doesn’t matter. In other words, if <code>AsOrdered</code> were the default, you’d need to apply <code>AsUnordered</code> to the majority of your parallel queries to get the best performance, which would be burdensome.</p>
</div></section>
<section data-pdf-bookmark="PLINQ Limitations" data-type="sect2"><div class="sect2" id="plinq_limitations">
<h2>PLINQ Limitations</h2>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="limitations" data-type="indexterm" id="id4470"/>There are practical limitations on what PLINQ can parallelize. The following query operators prevent parallelization by default unless the source elements are in their original indexing position:</p>
<ul class="simplelist">
<li><p>The indexed versions of <code>Select</code>, <code>SelectMany</code>, and <code>ElementAt</code></p></li>
</ul>
<p>Most query operators change the indexing position of elements (including those that remove elements, such as <code>Where</code>). This means that if you want to use the preceding operators, they’ll usually need to be at the start of the query.</p>
<p>The following query operators are parallelizable but use an expensive partitioning strategy that can sometimes be slower than sequential processing:</p>
<ul class="simplelist">
<li><p><code>Join</code>, <code>GroupBy</code>, <code>GroupJoin</code>, <code>Distinct</code>, <code>Union</code>, <code>Intersect</code>, and <code>Except</code></p></li>
</ul>
<p>The <code>Aggregate</code> operator’s <em>seeded</em> overloads in their standard incarnations are not parallelizable—PLINQ provides special overloads to deal with this (see <a data-type="xref" href="#optimizing_plinq">“Optimizing PLINQ”</a>).</p>
<p>All other operators are parallelizable, although use of these operators doesn’t guarantee that your query will be parallelized. PLINQ might run your query sequentially if it suspects that the overhead of parallelization will slow down that particular query. You can override this behavior and force parallelism by calling the following after <code>AsParallel()</code>:<a contenteditable="false" data-primary="" data-startref="ch22.html106" data-type="indexterm" id="id4471"/></p>
<pre data-type="programlisting">.WithExecutionMode (ParallelExecutionMode.ForceParallelism)</pre>
</div></section>
<section data-pdf-bookmark="Example: Parallel Spellchecker" data-type="sect2"><div class="sect2" id="example_parallel_spellchecker">
<h2>Example: Parallel Spellchecker</h2>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="parallel spellchecker example" data-type="indexterm" id="ch22.html107"/>Suppose that we want to write a spellchecker that runs quickly with very large documents by utilizing all available cores. By formulating our algorithm into a LINQ query, we can very easily parallelize it.</p>
<p>The first step is to download a dictionary of English words into a <code>HashSet</code> for efficient lookup:</p>
<pre data-type="programlisting">if (!File.Exists ("WordLookup.txt")    // Contains about 150,000 words
  File.WriteAllText ("WordLookup.txt",
    await new HttpClient().GetStringAsync (
      "http://www.albahari.com/ispell/allwords.txt"));

var wordLookup = new HashSet&lt;string&gt; (
  File.ReadAllLines ("WordLookup.txt"),
  StringComparer.InvariantCultureIgnoreCase);</pre>
<p>We then use our word lookup to create a test “document” comprising an array of a million random words. After we build the array, let’s introduce a couple of spelling mistakes:</p>
<pre data-type="programlisting">var random = new Random();
string[] wordList = wordLookup.ToArray();

string[] wordsToTest = Enumerable.Range (0, 1000000)
  .Select (i =&gt; wordList [random.Next (0, wordList.Length)])
  .ToArray();

wordsToTest [12345] = "woozsh";     // Introduce a couple
wordsToTest [23456] = "wubsie";     // of spelling mistakes.</pre>
<p>Now we can perform our parallel spellcheck by testing <code>wordsToTest</code> against <code>wordLookup</code>. PLINQ makes this very easy:</p>
<pre data-type="programlisting">var query = wordsToTest
  <strong>.AsParallel()</strong>
  .Select  ((word, index) =&gt; (word, index))
  .Where   (iword =&gt; !wordLookup.Contains (iword.word))
  .OrderBy (iword =&gt; iword.index);

foreach (var mistake in query)
  Console.WriteLine (mistake.word + " - index = " + mistake.index);

// OUTPUT:
// woozsh - index = 12345
// wubsie - index = 23456</pre>
<p>The <code>wordLookup.Contains</code> method in the predicate gives the query some “meat” and makes it worth parallelizing.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Notice that our query uses tuples <code>(word, index)</code> rather than anonymous types. Because tuples are implemented as value types rather than reference types, this improves peak memory consumption and performance by reducing heap allocations and subsequent garbage collections. (Benchmarking reveals the gains to be moderate in practice, due to the efficiency of the memory manager and the fact that the allocations in question don’t survive beyond Generation 0.)</p>
</div>
<section data-pdf-bookmark="Using ThreadLocal&lt;T&gt;" data-type="sect3"><div class="sect3" id="using_threadlocalless_thantgreater_than">
<h3>Using ThreadLocal&lt;T&gt;</h3>
<p><a contenteditable="false" data-primary="Thread..." data-secondary="ThreadLocal&lt;T&gt;" data-type="indexterm" id="id4472"/>Let’s extend our example by parallelizing the creation of the random test-word list itself. We structured this as a LINQ query, so it should be easy. Here’s the sequential version:</p>
<pre data-type="programlisting">string[] wordsToTest = Enumerable.Range (0, 1000000)
  .Select (i =&gt; wordList [<strong>random.Next</strong> (0, wordList.Length)])
  .ToArray();</pre>
<p>Unfortunately, the call to <code>random.Next</code> is not thread-safe, so it’s not as simple as inserting <code>AsParallel()</code> into the query. A potential solution is to write a function that locks around <code>random.Next</code>; however, this would limit concurrency. The better option is to use <code>ThreadLocal&lt;Random&gt;</code> (see <a data-type="xref" href="ch21.html#thread_local_storage">“Thread-Local Storage”</a>) to create a separate <code>Random</code> object for each thread. We then can parallelize the query, as follows:</p>
<pre data-type="programlisting">var localRandom = new ThreadLocal&lt;Random&gt;
 ( <strong>() =&gt; new Random (Guid.NewGuid().GetHashCode())</strong> );

string[] wordsToTest = Enumerable.Range (0, 1000000)<strong>.AsParallel()</strong>
  .Select (i =&gt; wordList [<strong>localRandom.Value.Next</strong> (0, wordList.Length)])
  .ToArray();</pre>
<p>In our factory function for instantiating a <code>Random</code> object, we pass in a <code>Guid</code>’s hashcode to ensure that if two <code>Random</code> objects are created within a short period of time, they’ll yield different random number sequences.<a contenteditable="false" data-primary="" data-startref="ch22.html107" data-type="indexterm" id="id4473"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="when_to_use_plinq">
<h1>When to Use PLINQ</h1>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="when to use" data-type="indexterm" id="id4474"/>It’s tempting to search your existing applications for LINQ queries and experiment with parallelizing them. This is usually unproductive, because most problems for which LINQ is obviously the best solution tend to execute very quickly and so don’t benefit from parallelization. A better approach is to find a CPU-intensive bottleneck and then consider whether it can be expressed as a LINQ query. (A welcome side effect of such restructuring is that LINQ typically makes code smaller and more readable.)</p>
<p>PLINQ is well suited to embarrassingly parallel problems. It can be a poor choice for imaging, however, because collating millions of pixels into an output sequence creates a bottleneck. Instead, it’s better to write pixels directly to an array or unmanaged memory block and use the <code>Parallel</code> class or task parallelism to manage the multithreading. (It is possible, however, to defeat result collation using <code>ForAll</code>—we discuss this in <a data-type="xref" href="#optimizing_plinq">“Optimizing PLINQ”</a>. Doing so makes sense if the image processing algorithm naturally lends itself to LINQ.)</p>
</div></aside>
</div></section>
</div></section>
<section data-pdf-bookmark="Functional Purity" data-type="sect2"><div class="sect2" id="functional_purity">
<h2>Functional Purity</h2>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="functional purity" data-type="indexterm" id="id4475"/><a contenteditable="false" data-primary="thread-unsafe operations" data-type="indexterm" id="id4476"/>Because PLINQ runs your query on parallel threads, you must be careful not to perform thread-unsafe operations. In particular, writing to variables is <em>side-effecting</em> and therefore thread-unsafe:</p>
<pre data-type="programlisting">// The following query multiplies each element by its position.
// Given an input of Enumerable.Range(0,999), it should output squares.
int i = 0;
var query = from n in Enumerable.Range(0,999).AsParallel() select n * <strong>i++</strong>;</pre>
<p>We could make incrementing <code>i</code> thread-safe by using locks, but the problem would still remain that <code>i</code> won’t necessarily correspond to the position of the input element. And adding <code>AsOrdered</code> to the query wouldn’t fix the latter problem, because <code>AsOrdered</code> ensures only that the elements are output in an order consistent with them having been processed sequentially—it doesn’t actually <em>process</em> them sequentially.</p>
<p>The correct solution is to rewrite our query to use the indexed version of <code>Select</code>:</p>
<pre data-type="programlisting">var query = Enumerable.Range(0,999).AsParallel().Select ((n, i) =&gt; n * i);</pre>
<p>For best performance, any methods called from query operators should be thread-safe by virtue of not writing to fields or properties (non-side-effecting, or <em>functionally pure</em>). If they’re thread-safe by virtue of <em>locking</em>, the query’s parallelism potential will be limited by the effects of contention.</p>
</div></section>
<section data-pdf-bookmark="Setting the Degree of Parallelism" data-type="sect2"><div class="sect2" id="setting_the_degree_of_parallelism">
<h2>Setting the Degree of Parallelism</h2>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="setting the degree of parallelism" data-type="indexterm" id="id4477"/>By default, PLINQ chooses an optimum degree of parallelism for the processor in use. <a contenteditable="false" data-primary="WithDegreeOfParallelism" data-type="indexterm" id="id4478"/>You can override it by calling <code>WithDegreeOfParallelism</code> after <code>AsParallel</code>:</p>
<pre data-type="programlisting">...AsParallel().<strong>WithDegreeOfParallelism(4)</strong>...</pre>
<p>An example of when you might increase the parallelism beyond the core count is with I/O-bound work (downloading many web pages at once, for instance). However, task combinators and asynchronous functions provide a similarly easy and more <em>efficient</em> solution (see <a data-type="xref" href="ch14.html#task_combinators">“Task Combinators”</a>). Unlike with <code>Task</code>s, PLINQ cannot perform I/O-bound work without blocking threads (and <em>pooled</em> threads, to make matters worse).</p>
<section data-pdf-bookmark="Changing the degree of parallelism" data-type="sect3"><div class="sect3" id="changing_the_degree_of_parallelism">
<h3>Changing the degree of parallelism</h3>
<p>You can call <code>WithDegreeOfParallelism</code> only once within a PLINQ query. If you need to call it again, you must force merging and repartitioning of the query by calling <code>AsParallel()</code> again within the query:</p>
<pre data-type="programlisting">"The Quick Brown Fox"
  .AsParallel().WithDegreeOfParallelism (2)
  .Where (c =&gt; !char.IsWhiteSpace (c))
  <strong>.AsParallel().WithDegreeOfParallelism (3)   // Forces Merge + Partition</strong>
  .Select (c =&gt; char.ToUpper (c))</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="Cancellation" data-type="sect2"><div class="sect2" id="cancellation-id00006">
<h2>Cancellation</h2>
<p><a contenteditable="false" data-primary="canceling a PLINQ query" data-type="indexterm" id="id4479"/><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="canceling a query" data-type="indexterm" id="id4480"/>Canceling a PLINQ query whose results you’re consuming in a <code>foreach</code> loop is easy: simply break out of the <code>foreach</code> and the query will be automatically canceled as the enumerator is implicitly disposed.</p>
<p><a contenteditable="false" data-primary="cancellation tokens" data-type="indexterm" id="id4481"/>For a query that terminates with a conversion, element, or aggregation operator, you can cancel it from another thread via a <em>cancellation token</em> (see <a data-type="xref" href="ch14.html#cancellation">“Cancellation”</a>). To insert a token, call <code>WithCancellation</code> after calling <code>AsParallel</code>, passing in the <code>Token</code> property of a <code>CancellationTokenSource</code> object. Another thread can then call <code>Cancel</code> on the token source (or we can call it ourselves with a delay). This then throws an <code>OperationCanceledException</code> on the query’s consumer:</p>
<pre data-type="programlisting">IEnumerable&lt;int&gt; tenMillion = Enumerable.Range (3, 10_000_000);

<strong>var cancelSource = new CancellationTokenSource();</strong>
<strong>cancelSource.CancelAfter (100);</strong>   // Cancel query after 100 milliseconds

var primeNumberQuery = 
  from n in tenMillion.AsParallel()<strong>.WithCancellation (cancelSource.Token)</strong>
  where Enumerable.Range (2, (int) Math.Sqrt (n)).All (i =&gt; n % i &gt; 0)
  select n;

try 
{
  // Start query running:
  int[] primes = primeNumberQuery.ToArray();
  // We'll never get here because the other thread will cancel us.
}
catch (<strong>OperationCanceledException</strong>)
{
  Console.WriteLine ("Query canceled");
}</pre>
<p>Upon cancellation, PLINQ waits for each worker thread to finish with its current element before ending the query. This means that any external methods that the query calls will run to completion.</p>
</div></section>
<section data-pdf-bookmark="Optimizing PLINQ" data-type="sect2"><div class="sect2" id="optimizing_plinq">
<h2>Optimizing PLINQ</h2>
<section data-pdf-bookmark="Output-side optimization" data-type="sect3"><div class="sect3" id="output_side_optimization">
<h3>Output-side optimization</h3>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="output-side optimization" data-type="indexterm" id="id4482"/><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="optimizing" data-type="indexterm" id="ch22.html108"/>One of PLINQ’s advantages is that it conveniently collates the results from parallelized work into a single output sequence. Sometimes, though, all that you end up doing with that sequence is running some function once over each element:</p>
<pre data-type="programlisting">foreach (int n in parallelQuery)
  DoSomething (n);</pre>
<p>If this is the case—and you don’t care about the order in which the elements are processed—you can improve efficiency with PLINQ’s <code>ForAll</code> method.</p>
<p>The <code>ForAll</code> method runs a delegate over every output element of a <code>ParallelQuery</code>. It hooks directly into PLINQ’s internals, bypassing the steps of collating and enumerating the results. Here’s a trivial example:</p>
<pre data-type="programlisting">"abcdef".AsParallel().Select (c =&gt; char.ToUpper(c)).ForAll (Console.Write);</pre>
<p><a data-type="xref" href="#plinq_forall">Figure 22-3</a> shows the process.</p>

<figure><div class="figure" id="plinq_forall">
<img alt="PLINQ ForAll" src="assets/cn10_2203.png"/>
<h6><span class="label">Figure 22-3. </span>PLINQ <code>ForAll</code></h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Collating and enumerating results is not a massively expensive operation, so the <code>ForAll</code> optimization yields the greatest gains when there are large numbers of quickly executing input elements.</p>
</div>
</div></section>
<section data-pdf-bookmark="Input-side optimization" data-type="sect3"><div class="sect3" id="input_side_optimization">
<h3>Input-side optimization</h3>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="input-side optimization" data-type="indexterm" id="ch22.html109"/><a contenteditable="false" data-primary="range partitioning" data-type="indexterm" id="ch22.html1010"/>PLINQ has three partitioning strategies for assigning input elements to threads:</p>
<table class="border">
<thead>
<tr>
<th>Strategy</th>
<th>Element allocation</th>
<th>Relative performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chunk partitioning</td>
<td>Dynamic</td>
<td>Average</td>
</tr>
<tr>
<td>Range partitioning</td>
<td>Static</td>
<td>Poor to excellent</td>
</tr>
<tr>
<td>Hash partitioning</td>
<td>Static</td>
<td>Poor</td>
</tr>
</tbody>
</table>
<p>For query operators that require comparing elements (<code>GroupBy</code>, <code>Join</code>, <code>GroupJoin</code>, <code>Intersect</code>, <code>Except</code>, <code>Union</code>, and <code>Distinct</code>), you have no choice: <a contenteditable="false" data-primary="hash partitioning" data-type="indexterm" id="id4483"/>PLINQ always uses <em>hash partitioning</em>. Hash partitioning is relatively inefficient in that it must precalculate the hashcode of every element (so that elements with identical hashcodes can be processed on the same thread). If you find this to be too slow, your only option is to call <code>AsSequential</code> to disable parallelization.</p>
<p>For all other query operators, you have a choice as to whether to use range or chunk partitioning. By default:</p>
<ul>
<li><p>If the input sequence is <em>indexable</em> (if it’s an array or implements <code>IList&lt;T&gt;</code>), PLINQ chooses <em>range partitioning</em>.</p></li>
<li><p>Otherwise, PLINQ chooses <em>chunk partitioning</em>.</p></li>
</ul>
<p>In a nutshell, range partitioning is faster with long sequences for which every element takes a similar amount of CPU time to process. Otherwise, chunk partitioning is usually faster.</p>
<p>To force range partitioning:</p>
<ul>
<li><p>If the query starts with <code>Enumerable.Range</code>, replace that method with <code>ParallelEnumerable.Range</code>.</p></li>
<li><p>Otherwise, simply call <code>ToList</code> or <code>ToArray</code> on the input sequence (obviously, this incurs a performance cost in itself, which you should take into account).</p></li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p><code>ParallelEnumerable.Range</code> is not simply a shortcut for calling <code>Enumerable.Range(</code>…<code>).AsParallel()</code>. It changes the performance of the query by activating range partitioning.</p>
</div>
<p>To force chunk partitioning, wrap the input sequence in a call to <code>Partitioner.Create</code> (in <code>System.Collection.Concurrent</code>), as follows:</p>
<pre data-type="programlisting">int[] numbers = { 3, 4, 5, 6, 7, 8, 9 };
var parallelQuery =
  <strong>Partitioner.Create (numbers, true)</strong>.AsParallel()
  .Where (...)</pre>
<p><a contenteditable="false" data-primary="chunk partitioning" data-type="indexterm" id="id4484"/>The second argument to <code>Partitioner.Create</code> indicates that you want to <em>load-balance</em> the query, which is another way of saying that you want chunk partitioning.</p>
<p>Chunk partitioning works by having each worker thread periodically grab small “chunks” of elements from the input sequence to process (see <a data-type="xref" href="#chunk_versus_range_partitioning">Figure 22-4</a>). PLINQ starts by allocating very small chunks (one or two elements at a time). It then increases the chunk size as the query progresses: this ensures that small sequences are effectively parallelized and large sequences don’t cause excessive round-tripping. If a worker happens to get “easy” elements (that process quickly), it will end up getting more chunks. This system keeps every thread equally busy (and the cores “balanced”); the only downside is that fetching elements from the shared input sequence requires synchronization (typically an exclusive lock)—and this can result in some overhead and contention.</p>

<p>Range partitioning bypasses the normal input-side enumeration and preallocates an equal number of elements to each worker, avoiding contention on the input sequence. But if some threads happen to get easy elements and finish early, they sit idle while the remaining threads continue working. Our earlier prime number calculator might perform poorly with range partitioning. An example of when range partitioning would do well is in calculating the sum of the square roots of the first 10 million integers:<a contenteditable="false" data-primary="" data-startref="ch22.html1010" data-type="indexterm" id="id4485"/><a contenteditable="false" data-primary="" data-startref="ch22.html109" data-type="indexterm" id="id4486"/></p>
<pre data-type="programlisting">ParallelEnumerable.Range (1, 10000000).Sum (i =&gt; Math.Sqrt (i))</pre>
<figure><div class="figure" id="chunk_versus_range_partitioning">
<img alt="Chunk versus range partitioning" src="assets/cn10_2204.png"/>
<h6><span class="label">Figure 22-4. </span>Chunk versus range partitioning</h6>
</div></figure>
<p><code>ParallelEnumerable.Range</code> returns a <code>ParallelQuery&lt;T&gt;</code>, so you don’t need to subsequently call <code>AsParallel</code>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Range partitioning doesn’t necessarily allocate element ranges in <em>contiguous</em> blocks—it might instead choose a “striping” strategy. For instance, if there are two workers, one worker might process odd-numbered elements while the other processes even-numbered elements. The <code>TakeWhile</code> operator is almost certain to trigger a striping strategy to avoid unnecessarily processing elements later in the sequence.</p>
</div>
</div></section>
<section data-pdf-bookmark="Optimizing custom aggregations" data-type="sect3"><div class="sect3" id="optimizing_custom_aggregations">
<h3>Optimizing custom aggregations</h3>
<p><a contenteditable="false" data-primary="PLINQ (Parallel LINQ)" data-secondary="custom aggregation optimization" data-type="indexterm" id="ch22.html1011"/>PLINQ parallelizes the <code>Sum</code>, <code>Average</code>, <code>Min</code>, and <code>Max</code> operators efficiently without additional intervention. <a contenteditable="false" data-primary="Aggregate operator" data-type="indexterm" id="id4487"/>The <code>Aggregate</code> operator, though, presents special challenges for PLINQ. As described in <a data-type="xref" href="ch09.html#linq_operators">Chapter 9</a>, <code>Aggregate</code> performs custom aggregations. For example, the following sums a sequence of numbers, mimicking the <code>Sum</code> operator:</p>
<pre data-type="programlisting">int[] numbers = { 1, 2, 3 };
int sum = numbers.Aggregate (0, (total, n) =&gt; total + n);   // 6</pre>
<p>We also saw in <a data-type="xref" href="ch09.html#linq_operators">Chapter 9</a> that for <em>unseeded</em> aggregations, the supplied delegate must be associative and commutative. PLINQ will give incorrect results if this rule is violated, because it draws <em>multiple seeds</em> from the input sequence in order to aggregate several partitions of the sequence simultaneously.</p>
<p>Explicitly seeded aggregations might seem like a safe option with PLINQ, but unfortunately these ordinarily execute sequentially because of the reliance on a single seed. <a contenteditable="false" data-primary="seed factory function" data-type="indexterm" id="id4488"/>To mitigate this, PLINQ provides another overload of <code>Aggregate</code> that lets you specify multiple seeds—or rather, a <em>seed factory function</em>. For each thread, it executes this function to generate a separate seed, which becomes a <em>thread-local</em> accumulator into which it locally aggregates elements.</p>
<p>You must also supply a function to indicate how to combine the local and main accumulators. Finally, this <code>Aggregate</code> overload (somewhat gratuitously) expects a delegate to perform any final transformation on the result (you can achieve this as easily by running some function on the result yourself afterward). So, here are the four delegates, in the order they are passed:</p>
<dl>
<dt><code>seedFactory</code></dt>
<dd>Returns a new local accumulator</dd>
<dt><code>updateAccumulatorFunc</code></dt>
<dd>Aggregates an element into a local accumulator</dd>
<dt><code>combineAccumulatorFunc</code></dt>
<dd>Combines a local accumulator with the main accumulator</dd>
<dt><code>resultSelector</code></dt>
<dd>Applies any final transformation on the end result</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a contenteditable="false" data-primary="seed value" data-type="indexterm" id="id4489"/>In simple scenarios, you can specify a <em>seed value</em> instead of a seed factory. This tactic fails when the seed is a reference type that you want to mutate, because the same instance will then be shared by each thread.</p>
</div>
<p>To give a very simple example, the following sums the values in a <code>numbers</code> array:</p>
<pre data-type="programlisting">numbers.AsParallel().Aggregate (
 () =&gt; 0,                                      // seedFactory
  (localTotal, n) =&gt; localTotal + n,           // updateAccumulatorFunc
  (mainTot, localTot) =&gt; mainTot + localTot,   // combineAccumulatorFunc
  finalResult =&gt; finalResult)                  // resultSelector</pre>
<p>This example is contrived in that we could get the same answer just as efficiently using simpler approaches (such as an unseeded aggregate, or better, the <code>Sum</code> operator). To give a more realistic example, suppose that we want to calculate the frequency of each letter in the English alphabet in a given string. A simple sequential solution might look like this:</p>
<pre data-type="programlisting">string text = "Let’s suppose this is a really long string";
var letterFrequencies = new int[26];
foreach (char c in text)
{
  int index = char.ToUpper (c) - 'A';
  if (index &gt;= 0 &amp;&amp; index &lt; 26) letterFrequencies [index]++;
};</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>An example of when the input text might be very long is in gene sequencing. The “alphabet” would then consist of the letters <em>a</em>, <em>c</em>, <em>g</em>, and <em>t</em>.</p>
</div>
<p>To parallelize this, we could replace the <code>foreach</code> statement with a call to <code>Parallel.ForEach</code> (which we cover in the following section), but this will leave us to deal with concurrency issues on the shared array. And locking around accessing that array would all but kill the potential for parallelization.</p>
<p><code>Aggregate</code> offers a tidy solution. The accumulator, in this case, is an array just like the <code>letterFrequencies</code> array in our preceding example. Here’s a sequential version using <code>Aggregate</code>:</p>
<pre data-type="programlisting">int[] result =
  text.Aggregate (
    new int[26],                // Create the "accumulator"
    (letterFrequencies, c) =&gt;   // Aggregate a letter into the accumulator
    {
      int index = char.ToUpper (c) - 'A';
      if (index &gt;= 0 &amp;&amp; index &lt; 26) letterFrequencies [index]++;
      return letterFrequencies;
    });</pre>
<p>And now the parallel version, using PLINQ’s special overload:</p>
<pre data-type="programlisting">int[] result =
  text.AsParallel().Aggregate (
   () =&gt; new int[26],             // Create a new local accumulator

    (localFrequencies, c) =&gt;       // Aggregate into the local accumulator
    {
      int index = char.ToUpper (c) - 'A';
      if (index &gt;= 0 &amp;&amp; index &lt; 26) localFrequencies [index]++;
      return localFrequencies;
    },
                                   // Aggregate local-&gt;main accumulator
    (mainFreq, localFreq) =&gt;
      mainFreq.Zip (localFreq, (f1, f2) =&gt; f1 + f2).ToArray(),

    finalResult =&gt; finalResult     // Perform any final transformation
  );                               // on the end result.</pre>
<p>Notice that the local accumulation function <em>mutates</em> the <code>localFrequencies</code> array. This ability to perform this optimization is important—and is legitimate because <code>localFrequencies</code> is local to each thread.<a contenteditable="false" data-primary="" data-startref="ch22.html1011" data-type="indexterm" id="id4490"/><a contenteditable="false" data-primary="" data-startref="ch22.html108" data-type="indexterm" id="id4491"/><a contenteditable="false" data-primary="" data-startref="ch22.html105" data-type="indexterm" id="id4492"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="The Parallel Class" data-type="sect1"><div class="sect1" id="the_parallel_class">
<h1>The Parallel Class</h1>
<p><a contenteditable="false" data-primary="Parallel class" data-type="indexterm" id="ch22.html1012"/><a contenteditable="false" data-primary="parallel programming" data-secondary="Parallel class" data-type="indexterm" id="ch22.html1013"/>PFX provides a basic form of structured parallelism via three static methods in the <code>Parallel</code> class:</p>
<dl>
<dt><code>Parallel.Invoke</code></dt>
<dd>Executes an array of delegates in parallel</dd>
<dt><code>Parallel.For</code></dt>
<dd>Performs the parallel equivalent of a C# <code>for</code> loop</dd>
<dt><code>Parallel.ForEach</code></dt>
<dd>Performs the parallel equivalent of a C# <code>foreach</code> loop</dd>
</dl>
<p>All three methods block until all work is complete. As with PLINQ, after an unhandled exception, remaining workers are stopped after their current iteration and the exception (or exceptions) are thrown back to the caller—wrapped in an <code>AggregateException</code> (see <a data-type="xref" href="#working_with_aggregateexception">“Working with AggregateException”</a>).</p>
<section data-pdf-bookmark="Parallel.Invoke" data-type="sect2"><div class="sect2" id="paralleldotinvoke">
<h2>Parallel.Invoke</h2>
<p><a contenteditable="false" data-primary="Parallel class" data-secondary="Parallel.Invoke" data-type="indexterm" id="id4493"/><a contenteditable="false" data-primary="Parallel.Invoke" data-type="indexterm" id="id4494"/><code>Parallel.Invoke</code> executes an array of <code>Action</code> delegates in parallel and then waits for them to complete. The simplest version of the method is defined as follows:</p>
<pre data-type="programlisting">public static void <strong>Invoke</strong> (params Action[] actions);</pre>
<p>Just as with PLINQ, the <code>Parallel</code>.* methods are optimized for compute-bound and not I/O-bound work. However, downloading two web pages at once provides a simple way to demonstrate <code>Parallel.Invoke</code>:</p>
<pre data-type="programlisting">Parallel.Invoke (
 () =&gt; new WebClient().DownloadFile ("http://www.linqpad.net", "lp.html"),
 () =&gt; new WebClient().DownloadFile ("http://microsoft.com", "ms.html"));</pre>
<p>On the surface, this seems like a convenient shortcut for creating and waiting on two thread-bound <code>Task</code> objects. But there’s an important difference: <span class="keep-together"><code>Parallel.Invoke</code></span> still works efficiently if you pass in an array of a million delegates. This is because it <em>partitions</em> large numbers of elements into batches that it assigns to a handful of underlying <code>Task</code>s rather than creating a separate <code>Task</code> for each delegate.</p>
<p>As with all of <code>Parallel</code>’s methods, you’re on your own when it comes to collating the results. This means that you need to keep thread safety in mind. The following, for instance, is thread-unsafe:</p>
<pre data-type="programlisting">var data = new List&lt;string&gt;();
Parallel.Invoke (
 () =&gt; <strong>data.Add</strong> (new WebClient().DownloadString ("http://www.foo.com")),
 () =&gt; <strong>data.Add</strong> (new WebClient().DownloadString ("http://www.far.com")));</pre>
<p>Locking around adding to the list would resolve this, although locking would create a bottleneck if you had a much larger array of quickly executing delegates. A better solution is to use the thread-safe collections, which we cover in later sections—<code>ConcurrentBag</code> would be ideal in this case.</p>
<p><code>Parallel.Invoke</code> is also overloaded to accept a <code>ParallelOptions</code> object:</p>
<pre data-type="programlisting">public static void <strong>Invoke</strong> (ParallelOptions options,
                           params Action[] actions);</pre>
<p>With <code>ParallelOptions</code>, you can insert a cancellation token, limit the maximum concurrency, and specify a custom task scheduler. A cancellation token is relevant when you’re executing (roughly) more tasks than you have cores: upon cancellation, any unstarted delegates will be abandoned. Any already executing delegates will, however, continue to completion. See <a data-type="xref" href="#cancellation-id00006">“Cancellation”</a> for an example of how to use cancellation tokens.</p>
</div></section>
<section data-pdf-bookmark="Parallel.For and Parallel.ForEach" data-type="sect2"><div class="sect2" id="paralleldotfor_and_paralleldotforeach">
<h2>Parallel.For and Parallel.ForEach</h2>
<p><a contenteditable="false" data-primary="Parallel class" data-secondary="Parallel.For and Parallel.ForEach" data-type="indexterm" id="ch22.html1014"/><a contenteditable="false" data-primary="Parallel.For" data-type="indexterm" id="ch22.html1015"/><a contenteditable="false" data-primary="Parallel.ForEach" data-type="indexterm" id="ch22.html1016"/><code>Parallel.For</code> and <code>Parallel.ForEach</code> perform the equivalent of a C# <code>for</code> and <code>foreach</code> loop but with each iteration executing in parallel instead of sequentially. Here are their (simplest) signatures:</p>
<pre data-type="programlisting">public static ParallelLoopResult <strong>For</strong> (
  int fromInclusive, int toExclusive, Action&lt;int&gt; body)

public static ParallelLoopResult <strong>ForEach</strong>&lt;TSource&gt; (
  IEnumerable&lt;TSource&gt; source, Action&lt;TSource&gt; body)</pre>
<p>This sequential <code>for</code> loop:</p>
<pre data-type="programlisting">for (int i = 0; i &lt; 100; i++)
  Foo (i);</pre>
<p>is parallelized like this:</p>
<pre data-type="programlisting"><strong>Parallel.For</strong> (0, 100, i =&gt; Foo (i));</pre>
<p>or more simply:</p>
<pre data-type="programlisting"><strong>Parallel.For</strong> (0, 100, Foo);</pre>
<p>And this sequential <code>foreach</code>:</p>
<pre data-type="programlisting">foreach (char c in "Hello, world")
  Foo (c);</pre>
<p>is parallelized like this:</p>
<pre data-type="programlisting">Parallel.ForEach ("Hello, world", Foo);</pre>
<p>To give a practical example, if we import the <code>System.Security.Cryptography</code> namespace, we can generate six public/private keypair strings in parallel, as follows:</p>
<pre data-type="programlisting">var keyPairs = new string[6];

Parallel.For (0, keyPairs.Length,
              i =&gt; keyPairs[i] = RSA.Create().ToXmlString (true));</pre>
<p>As with <code>Parallel.Invoke</code>, we can feed <code>Parallel.For</code> and <code>Parallel.ForEach</code> a large number of work items and they’ll be efficiently partitioned onto a few tasks.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The latter query could also be done with PLINQ:</p>
<pre data-type="programlisting">string[] keyPairs =
  ParallelEnumerable.Range (0, 6)
  .Select (i =&gt; RSA.Create().ToXmlString (true))
  .ToArray();</pre>
</div>
<section data-pdf-bookmark="Outer versus inner loops" data-type="sect3"><div class="sect3" id="outer_versus_inner_loops">
<h3>Outer versus inner loops</h3>
<p><a contenteditable="false" data-primary="Parallel.ForEach" data-secondary="outer versus inner loops" data-type="indexterm" id="id4495"/><code>Parallel.For</code> and <code>Parallel.ForEach</code> usually work best on outer rather than inner loops. This is because with the former, you’re offering larger chunks of work to parallelize, diluting the management overhead. Parallelizing both inner and outer loops is usually unnecessary.</p>

<p>In the following example, we’d typically need more than 100 cores to benefit from the inner parallelization:</p>
<pre data-type="programlisting">Parallel.For (0, 100, i =&gt;
{
  Parallel.For (0, 50, j =&gt; Foo (i, j));   // Sequential would be better
});                                        // for the inner loop.</pre>
</div></section>
<section data-pdf-bookmark="Indexed Parallel.ForEach" data-type="sect3"><div class="sect3" id="indexed_paralleldotforeach">
<h3>Indexed Parallel.ForEach</h3>
<p><a contenteditable="false" data-primary="loop iteration index" data-type="indexterm" id="id4496"/><a contenteditable="false" data-primary="Parallel.ForEach" data-secondary="indexed" data-type="indexterm" id="id4497"/>Sometimes, it’s useful to know the loop iteration index. With a sequential <code>foreach</code>, it’s easy:</p>
<pre data-type="programlisting"><strong>int i = 0;</strong>
<strong>foreach (char c in "Hello, world")</strong>
  Console.WriteLine (c.ToString() + <strong>i++</strong>);</pre>
<p>Incrementing a shared variable, however, is not thread-safe in a parallel context. You must instead use the following version of <code>ForEach</code>:</p>
<pre data-type="programlisting">public static ParallelLoopResult ForEach&lt;TSource&gt; (
  IEnumerable&lt;TSource&gt; source, Action&lt;TSource,ParallelLoopState<strong>,long</strong>&gt; body)</pre>
<p>We’ll ignore <code>ParallelLoopState</code> (which we cover in the following section). For now, we’re interested in <code>Action</code>’s third type parameter of type <code>long</code>, which indicates the loop index:</p>
<pre data-type="programlisting">Parallel.ForEach ("Hello, world", (c, state, i) =&gt;
{
   Console.WriteLine (c.ToString() + i);
});</pre>
<p>To put this into a practical context, let’s revisit the spellchecker that we wrote with PLINQ. The following code loads up a dictionary along with an array of a million words to test:</p>
<pre data-type="programlisting">if (!File.Exists ("WordLookup.txt"))    // Contains about 150,000 words
  new WebClient().DownloadFile (
    "http://www.albahari.com/ispell/allwords.txt", "WordLookup.txt");

var wordLookup = new HashSet&lt;string&gt; (
  File.ReadAllLines ("WordLookup.txt"),
  StringComparer.InvariantCultureIgnoreCase);

var random = new Random();
string[] wordList = wordLookup.ToArray();

string[] wordsToTest = Enumerable.Range (0, 1000000)
  .Select (i =&gt; wordList [random.Next (0, wordList.Length)])
  .ToArray();

wordsToTest [12345] = "woozsh";     // Introduce a couple
wordsToTest [23456] = "wubsie";     // of spelling mistakes.</pre>
<p>We can perform the spellcheck on our <code>wordsToTest</code> array using the indexed version of <code>Parallel.ForEach</code>, as follows:</p>
<pre data-type="programlisting">var misspellings = new ConcurrentBag&lt;Tuple&lt;int,string&gt;&gt;();

Parallel.ForEach (wordsToTest, (word, state, i) =&gt;
{
  if (!wordLookup.Contains (word))
    misspellings.Add (Tuple.Create ((int) i, word));
});</pre>
<p>Notice that we had to collate the results into a thread-safe collection: having to do this is the disadvantage when compared to using PLINQ. The advantage over PLINQ is that we avoid the cost of applying an indexed <code>Select</code> query operator—which is less efficient than an indexed <code>ForEach</code>.</p>
</div></section>
<section data-pdf-bookmark="ParallelLoopState: breaking early out of loops" data-type="sect3"><div class="sect3" id="parallelloopstate_breaking_early_out_of">
<h3>ParallelLoopState: breaking early out of loops</h3>
<p><a contenteditable="false" data-primary="Parallel.For" data-secondary="ParallelLoopState" data-type="indexterm" id="id4498"/><a contenteditable="false" data-primary="Parallel.ForEach" data-secondary="ParallelLoopState" data-type="indexterm" id="id4499"/><a contenteditable="false" data-primary="ParallelLoopState" data-type="indexterm" id="id4500"/>Because the loop body in a parallel <code>For</code> or <code>ForEach</code> is a delegate, you can’t exit the loop early with a <code>break</code> statement. Instead, you must call <code>Break</code> or <code>Stop</code> on a <code>ParallelLoopState</code> object:</p>
<pre data-type="programlisting">public class ParallelLoopState
{
  public void Break();
  public void Stop();

  public bool IsExceptional { get; }
  public bool IsStopped { get; }
  public long? LowestBreakIteration { get; }
  public bool ShouldExitCurrentIteration { get; }
}</pre>
<p>Obtaining a <code>ParallelLoopState</code> is easy: all versions of <code>For</code> and <code>ForEach</code> are overloaded to accept loop bodies of type <code>Action&lt;TSource,ParallelLoopState&gt;</code>. So, to parallelize this:</p>
<pre data-type="programlisting">foreach (char c in "Hello, world")
  if (c == ',')
    <strong>break;</strong>
  else
    Console.Write (c);</pre>
<p>do this:</p>
<pre data-type="programlisting">Parallel.ForEach ("Hello, world", <strong>(c, loopState)</strong> =&gt;
{
  if (c == ',')
    <strong>loopState.Break();</strong>
  else
    Console.Write (c);
});

// OUTPUT: Hlloe</pre>
<p>You can see from the output that loop bodies can complete in a random order. Aside from this difference, calling <code>Break</code> yields <em>at least</em> the same elements as executing the loop sequentially: this example will always output <em>at least</em> the letters <em>H</em>, <em>e</em>, <em>l</em>, <em>l</em>, and <em>o</em> in some order. In contrast, calling <code>Stop</code> instead of <code>Break</code> forces all threads to finish immediately after their current iteration. In our example, calling <code>Stop</code> could give us a subset of the letters <em>H</em>, <em>e</em>, <em>l</em>, <em>l</em>, and <em>o</em> if another thread were lagging behind. Calling <code>Stop</code> is useful when you’ve found something that you’re looking for—or when something has gone wrong and you won’t be looking at the results.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code>Parallel.For</code> and <code>Parallel.ForEach</code> methods return a <code>ParallelLoopResult</code> object that exposes properties called <code>IsCompleted</code> and <code>LowestBreakIteration</code>. These tell you whether the loop ran to completion; if it didn’t, it indicates at what cycle the loop was broken.</p>
<p>If <code>LowestBreakIteration</code> returns null, it means that you called <code>Stop</code> (rather than <code>Break</code>) on the loop.</p>
</div>
<p>If your loop body is long, you might want other threads to break partway through the method body in case of an early <code>Break</code> or <code>Stop</code>. You can do this by polling the <code>ShouldExitCurrentIteration</code> property at various places in your code; this property becomes true immediately after a <code>Stop</code>—or soon after a <code>Break</code>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><code>ShouldExitCurrentIteration</code> also becomes true after a cancellation request—or if an exception is thrown in the loop.</p>
</div>
<p><code>IsExceptional</code> lets you know whether an exception has occurred on another thread. Any unhandled exception will cause the loop to stop after each thread’s current iteration: to avoid this, you must explicitly handle exceptions in your code.</p>
</div></section>
<section class="pagebreak-before" data-pdf-bookmark="Optimization with local values" data-type="sect3"><div class="sect3" id="optimization_with_local_values">
<h3 class="less_space">Optimization with local values</h3>
<p><a contenteditable="false" data-primary="Parallel.For" data-secondary="optimization with local values" data-type="indexterm" id="ch22.html1017"/><a contenteditable="false" data-primary="Parallel.ForEach" data-secondary="optimization with local values" data-type="indexterm" id="ch22.html1018"/><code>Parallel.For</code> and <code>Parallel.ForEach</code> each offer a set of overloads that feature a generic type argument called <code>TLocal</code>. These overloads are designed to help you optimize the collation of data with iteration-intensive loops. The simplest is this:</p>
<pre data-type="programlisting">public static ParallelLoopResult For &lt;<strong>TLocal</strong>&gt; (
  int fromInclusive,
  int toExclusive,
  <strong>Func &lt;TLocal&gt; localInit,</strong>
  Func &lt;int, ParallelLoopState, <strong>TLocal, TLocal</strong>&gt; body,
  <strong>Action &lt;TLocal&gt; localFinally);</strong></pre>
<p>These methods are rarely needed in practice because their target scenarios are covered mostly by PLINQ (which is fortunate because these overloads are somewhat intimidating!).</p>
<p>Essentially, the problem is this: suppose that we want to sum the square roots of the numbers 1 through 10,000,000. Calculating 10 million square roots is easily parallelizable, but summing their values is troublesome because we must lock around updating the total:</p>
<pre data-type="programlisting">object locker = new object();
double total = 0;
Parallel.For (1, 10000000,
              i =&gt; { lock (locker) total += Math.Sqrt (i); });</pre>
<p>The gain from parallelization is more than offset by the cost of obtaining 10 million locks—plus the resultant blocking.</p>
<p>The reality, though, is that we don’t actually <em>need</em> 10 million locks. Imagine a team of volunteers picking up a large volume of litter. If all workers shared a single trash can, the travel and contention would make the process extremely inefficient. The obvious solution is for each worker to have a private or “local” trash can, which is occasionally emptied into the main bin.</p>
<p>The <code>TLocal</code> versions of <code>For</code> and <code>ForEach</code> work in exactly this way. The volunteers are internal worker threads, and the <em>local value</em> represents a local trash can. For <code>Parallel</code> to do this job, you must feed it two additional delegates that indicate the following:</p>
<ol>
<li><p>How to initialize a new local value</p></li>
<li><p>How to combine a local aggregation with the master value</p></li>
</ol>
<p>Additionally, instead of the body delegate returning <code>void</code>, it should return the new aggregate for the local value. Here’s our example refactored:</p>
<pre data-type="programlisting">object locker = new object();
double grandTotal = 0;

Parallel.For (1, 10000000,

  () =&gt; 0.0,                        // Initialize the local value.

  (i, state, localTotal) =&gt;         // Body delegate. Notice that it
     localTotal + Math.Sqrt (i),    // returns the new local total.

  localTotal =&gt;                                    // Add the local value
    { lock (locker) grandTotal += localTotal; }    // to the master value.
);</pre>
<p>We must still lock, but only around aggregating the local value to the grand total. This makes the process dramatically more efficient.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As stated earlier, PLINQ is often a good fit in these scenarios. Our example could be parallelized with PLINQ like this:</p>
<pre data-type="programlisting">ParallelEnumerable.Range (1, 10000000)
                  .Sum (i =&gt; Math.Sqrt (i))</pre>
<p><a contenteditable="false" data-primary="range partitioning" data-type="indexterm" id="id4501"/>(Notice that we used <code>ParallelEnumerable</code> to force <em>range partitioning</em>: this improves performance in this case because all numbers will take equally long to process.)</p>
<p>In more complex scenarios, you might use LINQ’s <code>Aggregate</code> operator instead of <code>Sum</code>. If you supplied a local seed factory, the situation would be somewhat analogous to providing a local value function with <code>Parallel.For</code>.<a contenteditable="false" data-primary="" data-startref="ch22.html1018" data-type="indexterm" id="id4502"/><a contenteditable="false" data-primary="" data-startref="ch22.html1017" data-type="indexterm" id="id4503"/> <a contenteditable="false" data-primary="" data-startref="ch22.html1016" data-type="indexterm" id="id4504"/><a contenteditable="false" data-primary="" data-startref="ch22.html1015" data-type="indexterm" id="id4505"/><a contenteditable="false" data-primary="" data-startref="ch22.html1014" data-type="indexterm" id="id4506"/><a contenteditable="false" data-primary="" data-startref="ch22.html1013" data-type="indexterm" id="id4507"/><a contenteditable="false" data-primary="" data-startref="ch22.html1012" data-type="indexterm" id="id4508"/></p>
</div>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Task Parallelism" data-type="sect1"><div class="sect1" id="task_parallelism">
<h1>Task Parallelism</h1>
<p><a contenteditable="false" data-primary="parallel programming" data-secondary="task parallelism" data-type="indexterm" id="ch22.html1019"/><a contenteditable="false" data-primary="task parallelism" data-type="indexterm" id="ch22.html1020"/><em>Task parallelism</em> is the lowest-level approach to parallelization with PFX. The classes for working at this level are defined in the <code>System.Threading.Tasks</code> namespace and comprise the following:</p>
<table class="border">
<thead>
<tr>
<th>Class</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Task</code></td>
<td>For managing a unit for work</td>
</tr>
<tr>
<td><code>Task&lt;TResult&gt;</code></td>
<td>For managing a unit for work with a return value</td>
</tr>
<tr>
<td><code>TaskFactory</code></td>
<td>For creating tasks</td>
</tr>
<tr>
<td><code>TaskFactory&lt;TResult&gt;</code></td>
<td>For creating tasks and continuations with the same return type</td>
</tr>
<tr>
<td><code>TaskScheduler</code></td>
<td>For managing the scheduling of tasks</td>
</tr>
<tr>
<td><code>TaskCompletionSource</code></td>
<td>For manually controlling a task’s workflow</td>
</tr>
</tbody>
</table>
<p>We covered the basics of tasks in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a>; in this section, we look at advanced features of tasks that are aimed at parallel programming:</p>
<ul>
<li><p>Tuning a task’s scheduling</p></li>
<li><p>Establish a parent/child relationship when one task is started from another</p></li>
<li><p>Advanced use of continuations</p></li>
<li><p><code>TaskFactory</code></p></li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The Task Parallel Library lets you create hundreds (or even thousands) of tasks with minimal overhead. But if you want to create millions of tasks, you’ll need to partition those tasks into larger work units to maintain efficiency. The <code>Parallel</code> class and PLINQ do this automatically.</p>
</div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Visual Studio provides a window for monitoring tasks (Debug®Window®Parallel Tasks). This is equivalent to the Threads window, but for tasks. The Parallel Stacks window also has a special mode for tasks.</p>
</div>
<section data-pdf-bookmark="Creating and Starting Tasks" data-type="sect2"><div class="sect2" id="creating_and_starting_tasks">
<h2>Creating and Starting Tasks</h2>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="creating and starting tasks" data-type="indexterm" id="id4509"/><a contenteditable="false" data-primary="Task..." data-secondary="Task.Run" data-type="indexterm" id="id4510"/>As described in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a>, <code>Task.Run</code> creates and starts a <code>Task</code> or <code>Task&lt;TResult&gt;</code>. This method is actually a shortcut for calling <code>Task.Factory.StartNew</code>, which allows greater flexibility through additional overloads.</p>
<section data-pdf-bookmark="Specifying a state object" data-type="sect3"><div class="sect3" id="specifying_a_state_object">
<h3>Specifying a state object</h3>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="specifying a state object" data-type="indexterm" id="id4511"/><a contenteditable="false" data-primary="Task..." data-secondary="Task.Factory.StartNew" data-type="indexterm" id="id4512"/><code>Task.Factory.StartNew</code> lets you specify a <em>state</em> object that is passed to the target. The target method’s signature must then comprise a single object-type parameter:</p>
<pre data-type="programlisting">var task = Task.Factory.StartNew (Greet<strong>, "Hello"</strong>);
task.Wait();  // Wait for task to complete.

void Greet (<strong>object state</strong>) { Console.Write (state); }   // Hello</pre>
<p>This avoids the cost of the closure required for executing a lambda expression that calls <code>Greet</code>. This is a micro-optimization and is rarely necessary in practice, so we can put the <em>state</em> object to better use, which is to assign a meaningful name to the task. We can then use the <code>AsyncState</code> property to query its name:</p>
<pre data-type="programlisting">var task = Task.Factory.StartNew (state =&gt; Greet ("Hello")<strong>, "Greeting"</strong>);
Console.WriteLine (task.AsyncState);   // <strong>Greeting</strong>
task.Wait();

void Greet (string message) { Console.Write (message); }</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Visual Studio displays each task’s <code>AsyncState</code> in the Parallel Tasks window, so having a meaningful name here can ease debugging considerably.</p>
</div>
</div></section>
<section data-pdf-bookmark="TaskCreationOptions" data-type="sect3"><div class="sect3" id="taskcreationoptions">
<h3>TaskCreationOptions</h3>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="TaskCreationOptions" data-type="indexterm" id="id4513"/><a contenteditable="false" data-primary="Task..." data-secondary="TaskCreationOptions" data-type="indexterm" id="id4514"/>You can tune a task’s execution by specifying a <code>TaskCreationOptions</code> enum when calling <code>StartNew</code> (or instantiating a <code>Task</code>). <code>TaskCreationOptions</code> is a flags enum with the following (combinable) values:</p>
<pre data-type="programlisting">LongRunning, PreferFairness, AttachedToParent</pre>
<p><code>LongRunning</code> suggests to the scheduler to dedicate a thread to the task, and as we described in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a>, this is beneficial for I/O-bound tasks and for long-running tasks that might otherwise force short-running tasks to wait an unreasonable amount of time before being scheduled.</p>
<p><code>PreferFairness</code> instructs the scheduler to try to ensure that tasks are scheduled in the order in which they were started. It might ordinarily do otherwise because it internally optimizes the scheduling of tasks using local work-stealing queues—an optimization that allows the creation of <em>child</em> tasks without incurring the contention overhead that would otherwise arise with a single work queue. A child task is created by specifying <code>AttachedToParent</code>.</p>
</div></section>
<section data-pdf-bookmark="Child tasks" data-type="sect3"><div class="sect3" id="child_tasks">
<h3>Child tasks</h3>
<p><a contenteditable="false" data-primary="child tasks" data-type="indexterm" id="id4515"/><a contenteditable="false" data-primary="task parallelism" data-secondary="child tasks" data-type="indexterm" id="id4516"/>When one task starts another, you can optionally establish a parent-child <span class="keep-together">relationship</span>:</p>
<pre data-type="programlisting">Task parent = Task.Factory.StartNew (() =&gt;
{
  Console.WriteLine ("I am a parent");

  Task.Factory.StartNew (() =&gt;        // Detached task
  {
    Console.WriteLine ("I am detached");
  });

  Task.Factory.StartNew (() =&gt;        // Child task
  {
    Console.WriteLine ("I am a child");
  }, TaskCreationOptions.<strong>AttachedToParent</strong>);
});</pre>
<p>A child task is special in that when you wait for the <em>parent</em> task to complete, it waits for any children, as well. At which point any child exceptions bubble up:</p>
<pre data-type="programlisting">TaskCreationOptions atp = TaskCreationOptions.AttachedToParent;
var parent = Task.Factory.StartNew (() =&gt; 
{
  Task.Factory.StartNew (() =&gt;   // Child
  {
    Task.Factory.StartNew (() =&gt; { throw null; }, atp);   // Grandchild
  }, atp);
});

// The following call throws a NullReferenceException (wrapped
// in nested AggregateExceptions):
parent.Wait();</pre>
<p>This can be particularly useful when a child task is a continuation, as you’ll see shortly.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Waiting on Multiple Tasks" data-type="sect2"><div class="sect2" id="waiting_on_multiple_tasks">
<h2>Waiting on Multiple Tasks</h2>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="waiting on multiple tasks" data-type="indexterm" id="id4517"/>We saw in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a> that you can wait on a single task either by calling its <code>Wait</code> method or by accessing its <code>Result</code> property (if it’s a <code>Task&lt;TResult&gt;</code>). <a contenteditable="false" data-primary="Task..." data-secondary="Task.WaitAll" data-type="indexterm" id="id4518"/><a contenteditable="false" data-primary="Task..." data-secondary="Task.WaitAny" data-type="indexterm" id="id4519"/>You can also wait on multiple tasks at once—via the static methods <code>Task.WaitAll</code> (waits for all the specified tasks to finish) and <code>Task.WaitAny</code> (waits for just one task to finish).</p>
<p><code>WaitAll</code> is similar to waiting out each task in turn, but is more efficient in that it requires (at most) just one context switch. Also, if one or more of the tasks throw an unhandled exception, <code>WaitAll</code> still waits out every task. It then rethrows an <code>AggregateException</code> that accumulates the exceptions from each faulted task (this is where <code>AggregateException</code> is genuinely useful). It’s equivalent to doing this:</p>
<pre data-type="programlisting">// Assume t1, t2 and t3 are tasks:
var exceptions = new List&lt;Exception&gt;();
try { t1.Wait(); } catch (AggregateException ex) { exceptions.Add (ex); }
try { t2.Wait(); } catch (AggregateException ex) { exceptions.Add (ex); }
try { t3.Wait(); } catch (AggregateException ex) { exceptions.Add (ex); }
if (exceptions.Count &gt; 0) throw new AggregateException (exceptions);</pre>
<p>Calling <code>WaitAny</code> is equivalent to waiting on a <code>ManualResetEventSlim</code> that’s signaled by each task as it finishes.</p>
<p>As well as a timeout, you can also pass in a <em>cancellation token</em> to the <code>Wait</code> methods: this lets you cancel the wait—<em>not the task itself</em>.</p>
</div></section>
<section data-pdf-bookmark="Canceling Tasks" data-type="sect2"><div class="sect2" id="canceling_tasks">
<h2>Canceling Tasks</h2>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="canceling tasks" data-type="indexterm" id="id4520"/>You can optionally pass in a cancellation token when starting a task. Then, if cancellation occurs via that token, the task itself enters the “Canceled” state:</p>
<pre data-type="programlisting">var cts = new CancellationTokenSource();
CancellationToken token = cts.Token;
cts.CancelAfter (500);

Task task = Task.Factory.StartNew (() =&gt; 
{
  Thread.Sleep (1000);
  token.ThrowIfCancellationRequested();  // Check for cancellation request
}, token);

try { task.Wait(); }
catch (AggregateException ex)
{
  Console.WriteLine (ex.InnerException is TaskCanceledException);  // True
  Console.WriteLine (task.IsCanceled);                             // True
  Console.WriteLine (task.Status);                             // Canceled
}</pre>
<p><a contenteditable="false" data-primary="OperationCanceledException" data-type="indexterm" id="id4521"/><a contenteditable="false" data-primary="Task..." data-secondary="TaskCanceledException" data-type="indexterm" id="id4522"/><code>TaskCanceledException</code> is a subclass of <code>OperationCanceledException</code>. If you want to explicitly throw an <code>OperationCanceledException</code> (rather than calling <code>token.ThrowIfCancellationRequested</code>), you must pass the cancellation token into <code>OperationCanceledException</code>’s constructor. If you fail to do this, the task won’t end up with a <code>TaskStatus.Canceled</code> status and won’t trigger <code>OnlyOnCanceled</code> <span class="keep-together">continuations</span>.</p>
<p>If the task is canceled before it has started, it won’t get scheduled—an <code>OperationCanceledException</code> will instead be thrown on the task immediately.</p>
<p>Because cancellation tokens are recognized by other APIs, you can pass them into other constructs and cancellations will propagate seamlessly:</p>
<pre data-type="programlisting">var cancelSource = new CancellationTokenSource();
CancellationToken token = cancelSource.Token;

Task task = Task.Factory.StartNew (() =&gt;
{
  // Pass our cancellation token into a PLINQ query:
  var query = <em>someSequence</em>.AsParallel().<strong>WithCancellation (token)</strong>...
  ... <em>enumerate query</em> ...
});</pre>
<p>Calling <code>Cancel</code> on <code>cancelSource</code> in this example will cancel the PLINQ query, which will throw an <code>OperationCanceledException</code> on the task body, which will then cancel the task.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The cancellation tokens that you can pass into methods such as <code>Wait</code> and <code>CancelAndWait</code> allow you to cancel the <em>wait</em> operation and not the task itself.</p>
</div>
</div></section>
<section data-pdf-bookmark="Continuations" data-type="sect2"><div class="sect2" id="continuations-id00008">
<h2>Continuations</h2>
<p><a contenteditable="false" data-primary="continuations" data-secondary="task parallelism" data-type="indexterm" id="ch22.html1021"/><a contenteditable="false" data-primary="ContinueWith method" data-type="indexterm" id="ch22.html1022"/><a contenteditable="false" data-primary="task parallelism" data-secondary="continuations" data-type="indexterm" id="ch22.html1023"/>The <code>ContinueWith</code> method executes a delegate immediately after a task ends:</p>
<pre data-type="programlisting">Task task1 = Task.Factory.StartNew (() =&gt; Console.Write ("antecedent.."));
Task task2 = task1.<strong>ContinueWith</strong> (ant =&gt; Console.Write ("..continuation"));</pre>
<p>As soon as <code>task1</code> (the <em>antecedent</em>) completes, fails, or is canceled, <code>task2</code> (the <em>continuation</em>) starts. (If <code>task1</code> had completed before the second line of code ran, <code>task2</code> would be scheduled to execute immediately.) The <code>ant</code> argument passed to the continuation’s lambda expression is a reference to the antecedent task. <code>ContinueWith</code> itself returns a task, making it easy to add further continuations.</p>
<p>By default, antecedent and continuation tasks may execute on different threads. You can force them to execute on the same thread by specifying <code>TaskContinuatio⁠n​Options.ExecuteSynchronously</code> when calling <code>ContinueWith</code>: this can improve performance in very fine-grained continuations by lessening indirection.</p>
<section data-pdf-bookmark="Continuations and Task&lt;TResult&gt;" data-type="sect3"><div class="sect3" id="continuations_and_taskless_thantresultg">
<h3>Continuations and Task&lt;TResult&gt;</h3>
<p><a contenteditable="false" data-primary="continuations" data-secondary="Task&lt;TResult&gt; and" data-type="indexterm" id="id4523"/><a contenteditable="false" data-primary="Task..." data-secondary="Task&lt;TResult&gt;" data-type="indexterm" id="id4524"/>Just like ordinary tasks, continuations can be of type <code>Task&lt;TResult&gt;</code> and return data. In the following example, we calculate <code>Math.Sqrt(8*2)</code> using a series of chained tasks and then write out the result:</p>
<pre data-type="programlisting">Task.Factory.StartNew&lt;int&gt; (() =&gt; 8)
  .ContinueWith (ant =&gt; ant.Result * 2)
  .ContinueWith (ant =&gt; Math.Sqrt (ant.Result))
  .ContinueWith (ant =&gt; Console.WriteLine (ant.Result));   // 4</pre>
<p>Our example is somewhat contrived for simplicity; in real life, these lambda expressions would call computationally intensive functions.</p>
</div></section>
<section data-pdf-bookmark="Continuations and exceptions" data-type="sect3"><div class="sect3" id="continuations_and_exceptions">
<h3>Continuations and exceptions</h3>
<p><a contenteditable="false" data-primary="continuations" data-secondary="exceptions and" data-type="indexterm" id="id4525"/><a contenteditable="false" data-primary="exceptions" data-secondary="continuations and" data-type="indexterm" id="id4526"/>A continuation can know whether an antecedent faulted by querying the antecedent task’s <code>Exception</code> property—or simply by invoking <code>Result</code> / <code>Wait</code> and catching the resultant <code>AggregateException</code>. If an antecedent faults and the continuation does neither, the exception is considered <em>unobserved</em> and the static <code>TaskScheduler​.Unob⁠servedTaskException</code> event fires when the task is later garbage-collected.</p>
<p>A safe pattern is to rethrow antecedent exceptions. As long as the continuation is <code>Wait</code>ed upon, the exception will be propagated and rethrown to the <code>Wait</code>er:</p>
<pre data-type="programlisting">Task continuation = Task.Factory.StartNew     (()  =&gt; { throw null; })
                                .ContinueWith (ant =&gt;
  {
    <strong>ant.Wait();</strong>
    // Continue processing...
  });

continuation.Wait();    // Exception is now thrown back to caller.</pre>
<p>Another way to deal with exceptions is to specify different continuations for exceptional versus nonexceptional outcomes. This is done with <code>TaskContinuationOptions</code>:</p>
<pre data-type="programlisting">Task task1 = Task.Factory.StartNew (() =&gt; { throw null; });

Task error = task1.ContinueWith (ant =&gt; Console.Write (ant.Exception),
                                 <strong>TaskContinuationOptions.OnlyOnFaulted</strong>);

Task ok = task1.ContinueWith (ant =&gt; Console.Write ("Success!"),
                              <strong>TaskContinuationOptions.NotOnFaulted</strong>);</pre>
<p>This pattern is particularly useful in conjunction with child tasks, as you’ll see very soon.</p>
<p>The following extension method “swallows” a task’s unhandled exceptions:</p>
<pre data-type="programlisting">public static void IgnoreExceptions (this Task task)
{
  task.ContinueWith (t =&gt; { var ignore = t.Exception; },
    TaskContinuationOptions.OnlyOnFaulted);
}</pre>
<p>(This could be improved by adding code to log the exception.) Here’s how it would be used:</p>
<pre data-type="programlisting">Task.Factory.StartNew (() =&gt; { throw null; }).IgnoreExceptions();</pre>
</div></section>
<section data-pdf-bookmark="Continuations and child tasks" data-type="sect3"><div class="sect3" id="continuations_and_child_tasks">
<h3>Continuations and child tasks</h3>
<p><a contenteditable="false" data-primary="child tasks" data-type="indexterm" id="id4527"/><a contenteditable="false" data-primary="continuations" data-secondary="child tasks and" data-type="indexterm" id="id4528"/>A powerful feature of continuations is that they kick off only when all child tasks have completed (see <a data-type="xref" href="#continuations-id00059">Figure 22-5</a>). At that point, any exceptions thrown by the children are marshaled to the continuation.</p>
<p>In the following example, we start three child tasks, each throwing a <code>NullReferen⁠ce​Exception</code>. We then catch all of them in one fell swoop via a continuation on the parent:</p>
<pre data-type="programlisting">TaskCreationOptions atp = TaskCreationOptions.AttachedToParent;
Task.Factory.StartNew (() =&gt;
{
  Task.Factory.StartNew (() =&gt; { throw null; }, atp);
  Task.Factory.StartNew (() =&gt; { throw null; }, atp);
  Task.Factory.StartNew (() =&gt; { throw null; }, atp);
})
.ContinueWith (p =&gt; Console.WriteLine (p.Exception),
                    TaskContinuationOptions.OnlyOnFaulted);</pre>
<figure><div class="figure" id="continuations-id00059">
<img alt="Continuations" src="assets/cn10_2205.png"/>
<h6><span class="label">Figure 22-5. </span>Continuations</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Conditional continuations" data-type="sect3"><div class="sect3" id="conditional_continuations-id00006">
<h3>Conditional continuations</h3>
<p><a contenteditable="false" data-primary="conditional continuations" data-type="indexterm" id="id4529"/><a contenteditable="false" data-primary="continuations" data-secondary="conditional" data-type="indexterm" id="id4530"/>By default, a continuation is scheduled <em>unconditionally</em>, whether the antecedent completes, throws an exception, or is canceled. You can alter this behavior via a set of (combinable) flags included within the <code>TaskContinuationOptions</code> enum. Following are the three core flags that control conditional continuation:</p>
<pre data-type="programlisting">NotOnRanToCompletion = 0x10000,
NotOnFaulted = 0x20000,
NotOnCanceled = 0x40000,</pre>
<p>These flags are subtractive in the sense that the more you apply, the less likely the continuation is to execute. For convenience, there are also the following precombined values:</p>
<pre data-type="programlisting"><strong>OnlyOnRanToCompletion</strong> = NotOnFaulted | NotOnCanceled,
<strong>OnlyOnFaulted</strong> = NotOnRanToCompletion | NotOnCanceled,
<strong>OnlyOnCanceled</strong> = NotOnRanToCompletion | NotOnFaulted</pre>
<p>(Combining all the <code>Not*</code> flags [<code>NotOnRanToCompletion</code>, <code>NotOnFaulted</code>, <code>NotOn​Can⁠celed</code>] is nonsensical because it would result in the continuation always being canceled.)</p>
<p>“RanToCompletion” means that the antecedent succeeded without cancellation or unhandled exceptions.</p>
<p>“Faulted” means that an unhandled exception was thrown on the antecedent.</p>
<p>“Canceled” means one of two things:</p>
<ul>
<li><p>The antecedent was canceled via its cancellation token. In other words, an <code>OperationCanceledException</code> was thrown on the antecedent, whose <code>CancellationToken</code> property matched that passed to the antecedent when it was started.</p></li>
<li><p>The antecedent was implicitly canceled because <em>it</em> didn’t satisfy a conditional continuation predicate.</p></li>
</ul>
<p>It’s essential to grasp that when a continuation doesn’t execute by virtue of these flags, the continuation is not forgotten or abandoned—it’s canceled. This means that any continuations on the continuation itself <em>will then run</em> unless you predicate them with <code>NotOnCanceled</code>. For example, consider this:</p>
<pre data-type="programlisting">Task t1 = Task.Factory.StartNew (...);

Task fault = t1.ContinueWith (ant =&gt; Console.WriteLine ("fault"),
                              TaskContinuationOptions.OnlyOnFaulted);

Task t3 = fault.ContinueWith (ant =&gt; Console.WriteLine ("t3"));</pre>
<p>As it stands, <code>t3</code> will always get scheduled—even if <code>t1</code> doesn’t throw an exception (see <a data-type="xref" href="#conditional_continuations-id00057">Figure 22-6</a>). This is because if <code>t1</code> succeeds, the <code>fault</code> task will be canceled, and with no continuation restrictions placed on <code>t3</code>, <code>t3</code> will then execute unconditionally.</p>

<p>If we want <code>t3</code> to execute only if <code>fault</code> actually runs, we must instead do this:</p>
<pre data-type="programlisting">Task t3 = fault.ContinueWith (ant =&gt; Console.WriteLine ("t3"),
                              <strong>TaskContinuationOptions.NotOnCanceled</strong>);</pre>
<p>(Alternatively, we could specify <code>OnlyOnRanToCompletion</code>; the difference is that <code>t3</code> would not then execute if an exception were thrown within <code>fault</code>.)</p>
<figure><div class="figure" id="conditional_continuations-id00057">
<img alt="Conditional continuations" src="assets/cn10_2206.png"/>
<h6><span class="label">Figure 22-6. </span>Conditional continuations</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Continuations with multiple antecedents" data-type="sect3"><div class="sect3" id="continuations_with_multiple_antecedents">
<h3>Continuations with multiple antecedents</h3>
<p><a contenteditable="false" data-primary="continuations" data-secondary="multiple antecedents with" data-type="indexterm" id="id4531"/>You can schedule continuation to execute based on the completion of multiple antecedents with the <code>ContinueWhenAll</code> and <code>ContinueWhenAny</code> methods in the <code>TaskFactory</code> class. These methods have become redundant, however, with the introduction of the task combinators discussed in <a data-type="xref" href="ch14.html#concurrency_and_asynchron">Chapter 14</a> (<code>WhenAll</code> and <code>WhenAny</code>). Specifically, given the following tasks:</p>
<pre data-type="programlisting">var task1 = Task.Run (() =&gt; Console.Write ("X"));
var task2 = Task.Run (() =&gt; Console.Write ("Y"));</pre>
<p>we can schedule a continuation to execute when both complete as follows:</p>
<pre data-type="programlisting">var continuation = Task.Factory.ContinueWhenAll (
  new[] { task1, task2 }, tasks =&gt; Console.WriteLine ("Done"));</pre>
<p>Here’s the same result with the <code>WhenAll</code> task combinator:</p>
<pre data-type="programlisting">var continuation = Task.WhenAll (task1, task2)
                       .ContinueWith (ant =&gt; Console.WriteLine ("Done"));</pre>
</div></section>
<section data-pdf-bookmark="Multiple continuations on a single antecedent" data-type="sect3"><div class="sect3" id="multiple_continuations_on_a_single_ante">
<h3>Multiple continuations on a single antecedent</h3>
<p><a contenteditable="false" data-primary="continuations" data-secondary="multiple continuations on a single antecedent" data-type="indexterm" id="id4532"/>Calling <code>ContinueWith</code> more than once on the same task creates multiple continuations on a single antecedent. When the antecedent finishes, all continuations will start together (unless you specify <code>TaskContinuationOptions.ExecuteSynchronously</code>, in which case the continuations will execute sequentially).</p>
<p>The following waits for one second and then writes either <code>XY</code> or <code>YX</code>:<a contenteditable="false" data-primary="" data-startref="ch22.html1023" data-type="indexterm" id="id4533"/><a contenteditable="false" data-primary="" data-startref="ch22.html1022" data-type="indexterm" id="id4534"/><a contenteditable="false" data-primary="" data-startref="ch22.html1021" data-type="indexterm" id="id4535"/></p>
<pre data-type="programlisting">var t = Task.Factory.StartNew (() =&gt; Thread.Sleep (1000));
t.ContinueWith (ant =&gt; Console.Write ("X"));
t.ContinueWith (ant =&gt; Console.Write ("Y"));</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="Task Schedulers" data-type="sect2"><div class="sect2" id="task_schedulers">
<h2>Task Schedulers</h2>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="task schedulers" data-type="indexterm" id="id4536"/><a contenteditable="false" data-primary="task schedulers" data-type="indexterm" id="id4537"/>A <em>task scheduler</em> allocates tasks to threads and is represented by the abstract <code>TaskScheduler</code> class. <a contenteditable="false" data-primary="default scheduler" data-type="indexterm" id="id4538"/>.NET provides two concrete implementations: the <em>default scheduler</em> that works in tandem with the CLR thread pool, and the <a contenteditable="false" data-primary="synchronization context scheduler" data-type="indexterm" id="id4539"/><em>synchronization context scheduler</em>. The latter is designed (primarily) to help you with the threading model of WPF and Windows Forms, which requires that user interface elements and controls are accessed only from the thread that created them (see <a data-type="xref" href="ch14.html#threading_in_rich_client_applications">“Threading in Rich Client Applications”</a>). By capturing it, we can instruct a task or a continuation to execute on this context:</p>
<pre data-type="programlisting">// Suppose we are on a UI thread in a Windows Forms / WPF application:
_uiScheduler = <strong>TaskScheduler.FromCurrentSynchronizationContext()</strong>;</pre>
<p>Assuming <code>Foo</code> is a compute-bound method that returns a string and <code>lblResult</code> is a WPF or Windows Forms label, we could then safely update the label after the operation completes, as follows:</p>
<pre data-type="programlisting">Task.Run (() =&gt; Foo())
  .ContinueWith (ant =&gt; lblResult.Content = ant.Result, <strong>_uiScheduler</strong>);</pre>
<p>Of course, C#’s asynchronous functions would more commonly be used for this kind of thing.</p>
<p>It’s also possible to write our own task scheduler (by subclassing <code>TaskScheduler</code>), although this is something you’d do only in very specialized scenarios. For custom scheduling, you’d more commonly use <code>TaskCompletionSource</code>.</p>
</div></section>
<section data-pdf-bookmark="TaskFactory" data-type="sect2"><div class="sect2" id="taskfactory">
<h2>TaskFactory</h2>
<p><a contenteditable="false" data-primary="task parallelism" data-secondary="TaskFactory" data-type="indexterm" id="id4540"/><a contenteditable="false" data-primary="Task..." data-secondary="Task.Factory" data-type="indexterm" id="id4541"/><a contenteditable="false" data-primary="Task..." data-secondary="TaskFactory object" data-type="indexterm" id="id4542"/>When you call <code>Task.Factory</code>, you’re calling a static property on <code>Task</code> that returns a default <code>TaskFactory</code> object. The purpose of a task factory is to create tasks; specifically, three kinds of tasks:</p>
<ul>
<li><p>“Ordinary” tasks (via <code>StartNew</code>)</p></li>
<li><p>Continuations with multiple antecedents (via <code>ContinueWhenAll</code> and <code>ContinueWhenAny</code>)</p></li>
<li><p>Tasks that wrap methods that follow the defunct APM (via <code>FromAsync</code>; see <a data-type="xref" href="ch14.html#obsolete_patterns">“Obsolete Patterns”</a>)</p></li>
</ul>
<p>Another way to create tasks is to instantiate <code>Task</code> and call <code>Start</code>. However, this lets you create only “ordinary” tasks, not continuations.</p>
<section data-pdf-bookmark="Creating your own task factories" data-type="sect3"><div class="sect3" id="creating_your_own_task_factories">
<h3>Creating your own task factories</h3>
<p><code>TaskFactory</code> is not an <em>abstract</em> factory: you can actually instantiate the class, and this is useful when you want to repeatedly create tasks using the same (nonstandard) values for <code>TaskCreationOptions</code>, <code>TaskContinuationOptions</code>, or <code>TaskScheduler</code>. For example, if we want to repeatedly create long-running <em>parented</em> tasks, we could create a custom factory, as follows:</p>
<pre data-type="programlisting">var factory = new TaskFactory (
  TaskCreationOptions.LongRunning | TaskCreationOptions.AttachedToParent,
  TaskContinuationOptions.None);</pre>
<p class="pagebreak-before">Creating tasks is then simply a matter of calling <code>StartNew</code> on the factory:</p>
<pre data-type="programlisting">Task task1 = factory.StartNew (Method1);
Task task2 = factory.StartNew (Method2);
...</pre>
<p>The custom continuation options are applied when calling <code>ContinueWhenAll</code> and <code>ContinueWhenAny</code>.<a contenteditable="false" data-primary="" data-startref="ch22.html1020" data-type="indexterm" id="id4543"/><a contenteditable="false" data-primary="" data-startref="ch22.html1019" data-type="indexterm" id="id4544"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Working with AggregateException" data-type="sect1"><div class="sect1" id="working_with_aggregateexception">
<h1>Working with AggregateException</h1>
<p><a contenteditable="false" data-primary="AggregateException class" data-type="indexterm" id="ch22.html1024"/><a contenteditable="false" data-primary="AggregateException class" data-secondary="parallel programming" data-type="indexterm" id="ch22.html1025"/><a contenteditable="false" data-primary="parallel programming" data-secondary="AggregateException and" data-type="indexterm" id="ch22.html1026"/>As we’ve seen, PLINQ, the <code>Parallel</code> class, and <code>Task</code>s automatically marshal exceptions to the consumer. To see why this is essential, consider the following LINQ query, which throws a <code>DivideByZeroException</code> on the first iteration:</p>
<pre data-type="programlisting">try
{
  var query = from i in Enumerable.Range (0, 1000000)
              select 100 / i;
  ...
}
catch (DivideByZeroException)
{
  ...
}</pre>
<p>If we asked PLINQ to parallelize this query and it ignored the handling of exceptions, a <code>DivideByZeroException</code> would probably be thrown on a <em>separate thread</em>, bypassing our <code>catch</code> block and causing the application to die.</p>
<p>Hence, exceptions are automatically caught and rethrown to the caller. But unfortunately, it’s not quite as simple as catching a <code>DivideByZeroException</code>. Because these libraries utilize many threads, it’s actually possible for two or more exceptions to be thrown simultaneously. To ensure that all exceptions are reported, exceptions are therefore wrapped in an <code>AggregateException</code> container, which exposes an <code>InnerExceptions</code> property containing each of the caught exception(s):</p>
<pre data-type="programlisting">try
{
  var query = from i in ParallelEnumerable.Range (0, 1000000)
              select 100 / i;
  // Enumerate query
  ...
}
catch (<strong>AggregateException</strong> aex)
{
  foreach (Exception ex in aex.<strong>InnerExceptions</strong>)
    Console.WriteLine (ex.Message);
}</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Both PLINQ and the <code>Parallel</code> class end the query or loop execution upon encountering the first exception—by not processing any further elements or loop bodies. More exceptions might be thrown, however, before the current cycle is complete. The first exception in <code>AggregateException</code> is visible in the <code>InnerException</code> property.</p>
</div>
<section data-pdf-bookmark="Flatten and Handle" data-type="sect2"><div class="sect2" id="flatten_and_handle">
<h2>Flatten and Handle</h2>
<p>The <code>AggregateException</code> class provides a couple of methods to simplify exception handling: <code>Flatten</code> and <code>Handle</code>.</p>
<section data-pdf-bookmark="Flatten" data-type="sect3"><div class="sect3" id="flatten">
<h3>Flatten</h3>
<p><a contenteditable="false" data-primary="AggregateException class" data-secondary="Flatten method" data-type="indexterm" id="id4545"/><a contenteditable="false" data-primary="Flatten method" data-type="indexterm" id="id4546"/><code>AggregateException</code>s will quite often contain other <code>AggregateException</code>s. An example of when this might happen is if a child task throws an exception. You can eliminate any level of nesting to simplify handling by calling <code>Flatten</code>. This method returns a new <code>AggregateException</code> with a simple flat list of inner exceptions:</p>
<pre data-type="programlisting">catch (AggregateException aex)
{
  foreach (Exception ex in aex.<strong>Flatten</strong>().InnerExceptions)
    myLogWriter.LogException (ex);
}</pre>
</div></section>
<section data-pdf-bookmark="Handle" data-type="sect3"><div class="sect3" id="handle">
<h3>Handle</h3>
<p><a contenteditable="false" data-primary="AggregateException class" data-secondary="Handle method" data-type="indexterm" id="id4547"/><a contenteditable="false" data-primary="Handle method" data-type="indexterm" id="id4548"/>Sometimes, it’s useful to catch only specific exception types, and have other types rethrown. The <code>Handle</code> method on <code>AggregateException</code> provides a shortcut for doing this. It accepts an exception predicate which it runs over every inner exception:</p>
<pre data-type="programlisting">public void Handle (Func&lt;Exception, bool&gt; predicate)</pre>
<p>If the predicate returns <code>true</code>, it considers that exception “handled.” After the delegate has run over every exception, the following happens:</p>
<ul>
<li><p>If all exceptions were “handled” (the delegate returned <code>true</code>), the exception is not rethrown.</p></li>
<li><p>If there were any exceptions for which the delegate returned <code>false</code> (“unhandled”), a new <code>AggregateException</code> is built up containing those exceptions and is rethrown.</p></li>
</ul>
<p>For instance, the following ends up rethrowing another <code>AggregateException</code> that contains a single <code>NullReferenceException</code>:<a contenteditable="false" data-primary="" data-startref="ch22.html1026" data-type="indexterm" id="id4549"/><a contenteditable="false" data-primary="" data-startref="ch22.html1025" data-type="indexterm" id="id4550"/><a contenteditable="false" data-primary="" data-startref="ch22.html1024" data-type="indexterm" id="id4551"/></p>
<pre data-type="programlisting">var parent = Task.Factory.StartNew (() =&gt; 
{
  // We’ll throw 3 exceptions at once using 3 child tasks:

  int[] numbers = { 0 };

  var childFactory = new TaskFactory
   (TaskCreationOptions.AttachedToParent, TaskContinuationOptions.None);

  childFactory.StartNew (() =&gt; 5 / numbers[0]);   // Division by zero
  childFactory.StartNew (() =&gt; numbers [1]);      // Index out of range
  childFactory.StartNew (() =&gt; { throw null; });  // Null reference
});

try { parent.Wait(); }
catch (AggregateException aex)
{
  <strong>aex.Flatten().Handle</strong> (ex =&gt;   // Note that we still need to call Flatten
  {
    if (ex is DivideByZeroException)
    {
      Console.WriteLine ("Divide by zero");
      return true;                           // This exception is "handled"
    }
    if (ex is IndexOutOfRangeException)
    {
      Console.WriteLine ("Index out of range");
      return true;                           // This exception is "handled"   
    }
    return false;    // All other exceptions will get rethrown
  });
}</pre>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Concurrent Collections" data-type="sect1"><div class="sect1" id="concurrent_collections">
<h1>Concurrent Collections</h1>
<p><a contenteditable="false" data-primary="System..." data-secondary="System.Collections.Concurrent" data-type="indexterm" id="id4552"/><a contenteditable="false" data-primary="concurrent collections" data-type="indexterm" id="ch22.html1027"/><a contenteditable="false" data-primary="parallel programming" data-secondary="concurrent collections" data-type="indexterm" id="ch22.html1028"/>.NET offers thread-safe collections in the <code>System.Collections.Concurrent</code> <span class="keep-together">namespace</span>:</p>
<table class="border">
<thead>
<tr>
<th>Concurrent collection</th>
<th>Nonconcurrent equivalent</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ConcurrentStack&lt;T&gt;</code></td>
<td><code>Stack&lt;T&gt;</code></td>
</tr>
<tr>
<td><code>ConcurrentQueue&lt;T&gt;</code></td>
<td><code>Queue&lt;T&gt;</code></td>
</tr>
<tr>
<td><code>ConcurrentBag&lt;T&gt;</code></td>
<td>(none)</td>
</tr>
<tr>
<td><code>ConcurrentDictionary&lt;TKey,TValue&gt;</code></td>
<td><code>Dictionary&lt;TKey,TValue&gt;</code></td>
</tr>
</tbody>
</table>
<p>The concurrent collections are optimized for high-concurrency scenarios; however, they can also be useful whenever you need a thread-safe collection (as an alternative to locking around an ordinary collection). There are some caveats, though:</p>
<ul>
<li><p>The conventional collections outperform the concurrent collections in all but highly concurrent scenarios.</p></li>
<li><p>A thread-safe collection doesn’t guarantee that the code using it will be thread-safe (see <a data-type="xref" href="ch21.html#locking_and_thread_safet">“Locking and Thread Safety”</a>).</p></li>
<li><p>If you enumerate over a concurrent collection while another thread is modifying it, no exception is thrown—instead, you get a mixture of old and new content.</p></li>
<li><p>There’s no concurrent version of <code>List&lt;T&gt;</code>.</p></li>
<li><p>The concurrent stack, queue, and bag classes are implemented internally with linked lists. This makes them less memory-efficient than the nonconcurrent <code>Stack</code> and <code>Queue</code> classes, but better for concurrent access because linked lists are conducive to lock-free or low-lock implementations. (This is because inserting a node into a linked list requires updating just a couple of references, whereas inserting an element into a <code>List&lt;T&gt;</code>-like structure might require moving thousands of existing elements.)</p></li>
</ul>
<p>In other words, these collections are not merely shortcuts for using an ordinary collection with a lock. To demonstrate, if we execute the following code on a <em>single</em> thread:</p>
<pre data-type="programlisting">var d = new ConcurrentDictionary&lt;int,int&gt;();
for (int i = 0; i &lt; 1000000; i++) d[i] = 123;</pre>
<p>it runs three times more slowly than this:</p>
<pre data-type="programlisting">var d = new Dictionary&lt;int,int&gt;();
for (int i = 0; i &lt; 1000000; i++) <strong>lock (d)</strong> d[i] = 123;</pre>
<p>(<em>Reading</em> from a <code>ConcurrentDictionary</code>, however, is fast because reads are lock-free.)</p>
<p>The concurrent collections also differ from conventional collections in that they expose special methods to perform atomic test-and-act operations, such as <code>TryPop</code>. Most of these methods are unified via the <code>IProducerConsumerCollection&lt;T&gt;</code> interface.</p>
<section data-pdf-bookmark="IProducerConsumerCollection&lt;T&gt;" data-type="sect2"><div class="sect2" id="iproducerconsumercollectionless_thantgr">
<h2>IProducerConsumerCollection&lt;T&gt;</h2>
<p><a contenteditable="false" data-primary="concurrent collections" data-secondary="IProducerConsumerCollection&lt;T&gt;" data-type="indexterm" id="id4553"/><a contenteditable="false" data-primary="IProducerConsumerCollection&lt;T&gt;" data-type="indexterm" id="id4554"/><a contenteditable="false" data-primary="producer/consumer collection" data-type="indexterm" id="id4555"/>A producer/consumer collection is one for which the two primary use cases are:</p>
<ul>
<li><p>Adding an element (“producing”)</p></li>
<li><p>Retrieving an element while removing it (“consuming”)</p></li>
</ul>
<p>The classic examples are stacks and queues. Producer/consumer collections are significant in parallel programming because they’re conducive to efficient lock-free implementations.</p>
<p>The <code>IProducerConsumerCollection&lt;T&gt;</code> interface represents a thread-safe producer/consumer collection. The following classes implement this interface:</p>
<pre data-type="programlisting">ConcurrentStack&lt;T&gt;
ConcurrentQueue&lt;T&gt;
ConcurrentBag&lt;T&gt;</pre>
<p><code>IProducerConsumerCollection&lt;T&gt;</code> extends <code>ICollection</code>, adding the following methods:</p>
<pre data-type="programlisting">void CopyTo (T[] array, int index);
T[] ToArray();
<strong>bool TryAdd (T item);</strong>
<strong>bool TryTake (out T item);</strong></pre>
<p>The <code>TryAdd</code> and <code>TryTake</code> methods test whether an add/remove operation can be performed; if so, they perform the add/remove. The testing and acting are atomically performed, eliminating the need to lock as you would around a conventional collection:</p>
<pre data-type="programlisting">int result;
<strong>lock (myStack)</strong> if (myStack.Count &gt; 0) result = myStack.Pop();</pre>
<p><code>TryTake</code> returns <code>false</code> if the collection is empty. <code>TryAdd</code> always succeeds and returns <code>true</code> in the three implementations provided. If you wrote your own concurrent collection that prohibited duplicates, however, you’d make <code>TryAdd</code> return <code>false</code> if the element already existed (an example would be if you wrote a concurrent <em>set</em>).</p>
<p>The particular element that <code>TryTake</code> removes is defined by the subclass:</p>
<ul>
<li><p>With a stack, <code>TryTake</code> removes the most recently added element.</p></li>
<li><p>With a queue, <code>TryTake</code> removes the least recently added element.</p></li>
<li><p>With a bag, <code>TryTake</code> removes whatever element it can remove most efficiently.</p></li>
</ul>
<p>The three concrete classes mostly implement the <code>TryTake</code> and <code>TryAdd</code> methods explicitly, exposing the same functionality through more specifically named public methods such as <code>TryDequeue</code> and <code>TryPop</code>.</p>
</div></section>
<section data-pdf-bookmark="ConcurrentBag&lt;T&gt;" data-type="sect2"><div class="sect2" id="concurrentbagless_thantgreater_than">
<h2>ConcurrentBag&lt;T&gt;</h2>
<p><a contenteditable="false" data-primary="concurrent collections" data-secondary="ConcurrentBag&lt;T&gt;" data-type="indexterm" id="id4556"/><a contenteditable="false" data-primary="ConcurrentBag&lt;T&gt;" data-type="indexterm" id="id4557"/><code>ConcurrentBag&lt;T&gt;</code> stores an <em>unordered</em> collection of objects (with duplicates permitted). <code>ConcurrentBag&lt;T&gt;</code> is suitable in situations for which you <em>don’t care</em> which element you get when calling <code>Take</code> or <code>TryTake</code>.</p>
<p>The benefit of <code>ConcurrentBag&lt;T&gt;</code> over a concurrent queue or stack is that a bag’s <code>Add</code> method suffers almost <em>no</em> contention when called by many threads at once. In contrast, calling <code>Add</code> in parallel on a queue or stack incurs <em>some</em> contention (although a lot less than locking around a <em>nonconcurrent</em> collection). Calling <code>Take</code> on a concurrent bag is also very efficient—as long as each thread doesn’t take more elements than it <code>Add</code>ed.</p>
<p>Inside a concurrent bag, each thread gets its own private linked list. Elements are added to the private list that belongs to the thread calling <code>Add</code>, eliminating contention. When you enumerate over the bag, the enumerator travels through each thread’s private list, yielding each of its elements in turn.</p>
<p>When you call <code>Take</code>, the bag first looks at the current thread’s private list. If there’s at least one element,<sup><a data-type="noteref" href="ch22.html#ch12fn1" id="ch12fn1-marker">1</a></sup> it can complete the task easily and without contention. But if the list is empty, it must “steal” an element from another thread’s private list and incur the potential for contention.</p>
<p>So, to be precise, calling <code>Take</code> gives you the element added most recently on that thread; if there are no elements on that thread, it gives you the element added most recently on another thread, chosen at random.</p>
<p>Concurrent bags are ideal when the parallel operation on your collection mostly comprises <code>Add</code>ing elements—or when the <code>Add</code>s and <code>Take</code>s are balanced on a thread. We saw an example of the former previously, when using <code>Parallel.ForEach</code> to implement a parallel spellchecker:</p>
<pre data-type="programlisting">var misspellings = new ConcurrentBag&lt;Tuple&lt;int,string&gt;&gt;();

Parallel.ForEach (wordsToTest, (word, state, i) =&gt;
{
  if (!wordLookup.Contains (word))
    misspellings.Add (Tuple.Create ((int) i, word));
});</pre>
<p>A concurrent bag would be a poor choice for a producer/consumer queue because elements are added and removed by <em>different</em> threads.<a contenteditable="false" data-primary="" data-startref="ch22.html1028" data-type="indexterm" id="id4558"/><a contenteditable="false" data-primary="" data-startref="ch22.html1027" data-type="indexterm" id="id4559"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="BlockingCollection&lt;T&gt;" data-type="sect1"><div class="sect1" id="blockingcollectionless_thantgreater_tha">
<h1>BlockingCollection&lt;T&gt;</h1>
<p><a contenteditable="false" data-primary="BlockingCollection&lt;T&gt;" data-type="indexterm" id="ch22.html1029"/><a contenteditable="false" data-primary="parallel programming" data-secondary="BlockingCollection&lt;T&gt;" data-type="indexterm" id="ch22.html1030"/>If you call <code>TryTake</code> on any of the producer/consumer collections we discussed in the previous section, <code>ConcurrentStack&lt;T&gt;</code>, <code>ConcurrentQueue&lt;T&gt;</code>, and <code>ConcurrentBag&lt;T&gt;</code>, and the collection is empty, the method returns <code>false</code>. Sometimes, it would be more useful in this scenario to <em>wait</em> until an element is available.</p>
<p>Rather than overloading the <code>TryTake</code> methods with this functionality (which would have caused a blowout of members after allowing for cancellation tokens and timeouts), PFX’s designers encapsulated this functionality into a wrapper class called <code>BlockingCollection&lt;T&gt;</code>. A blocking collection wraps any collection that implements <code>IProducerConsumerCollection&lt;T&gt;</code> and lets you <code>Take</code> an element from the wrapped collection—blocking if no element is available.</p>
<p>A blocking collection also lets you limit the total size of the collection, blocking the <em>producer</em> if that size is exceeded. A collection limited in this manner is called a <em>bounded blocking collection</em>.</p>
<p class="pagebreak-before">To use <code>BlockingCollection&lt;T&gt;</code>:</p>
<ol>
<li><p>Instantiate the class, optionally specifying the <code>IProducerConsumerCollection&lt;T&gt;</code> to wrap and the maximum size (bound) of the collection.</p></li>
<li><p>Call <code>Add</code> or <code>TryAdd</code> to add elements to the underlying collection.</p></li>
<li><p>Call <code>Take</code> or <code>TryTake</code> to remove (consume) elements from the underlying collection.</p></li>
</ol>
<p>If you call the constructor without passing in a collection, the class will automatically instantiate a <code>ConcurrentQueue&lt;T&gt;</code>. The producing and consuming methods let you specify cancellation tokens and timeouts. <code>Add</code> and <code>TryAdd</code> may block if the collection size is bounded; <code>Take</code> and <code>TryTake</code> block while the collection is empty.</p>
<p>Another way to consume elements is to call <code>GetConsumingEnumerable</code>. This returns a (potentially) infinite sequence that yields elements as they become available. You can force the sequence to end by calling <code>CompleteAdding</code>: this method also prevents further elements from being enqueued.</p>
<p><code>BlockingCollection</code> also provides static methods called <code>AddToAny</code> and <code>TakeFro⁠m​Any</code>, which let you add or take an element while specifying several blocking collections. The action is then honored by the first collection able to service the request.</p>
<section data-pdf-bookmark="Writing a Producer/Consumer Queue" data-type="sect2"><div class="sect2" id="writing_a_producersolidusconsumer_queue">
<h2>Writing a Producer/Consumer Queue</h2>
<p><a contenteditable="false" data-primary="BlockingCollection&lt;T&gt;" data-secondary="writing a producer/consumer queue" data-type="indexterm" id="ch22.html1031"/><a contenteditable="false" data-primary="producer/consumer queue" data-type="indexterm" id="ch22.html1032"/>A producer/consumer queue is a useful structure, both in parallel programming and general concurrency scenarios. Here’s how it works:</p>
<ul>
<li><p>A queue is set up to describe work items—or data upon which work is performed.</p></li>
<li><p>When a task needs executing, it’s enqueued, and the caller gets on with other things.</p></li>
<li><p>One or more worker threads plug away in the background, picking off and executing queued items.</p></li>
</ul>
<p>A producer/consumer queue gives you precise control over how many worker threads execute at once, which is useful not only in limiting CPU consumption but other resources, as well. If the tasks perform intensive disk I/O, for instance, you can limit concurrency to avoid starving the operating system and other applications. You can also dynamically add and remove workers throughout the queue’s life. The CLR’s thread pool itself is a kind of producer/consumer queue, optimized for short-running compute-bound jobs.</p>
<p>A producer/consumer queue typically holds items of data upon which (the same) task is performed. For example, the items of data may be filenames, and the task might be to encrypt those files. By making the item a delegate, however, you can write a more general-purpose producer/consumer queue where each item can do anything.</p>
<p>At <a href="http://albahari.com/threading"><em class="hyperlink">http://albahari.com/threading</em></a>, we show how to write a producer/consumer queue from scratch using an <code>AutoResetEvent</code> (and later, using <code>Monitor</code>’s <code>Wait</code> and <code>Pulse</code>). However, writing a producer/consumer from scratch is unnecessary because most of the functionality is provided by <code>BlockingCollection&lt;T&gt;</code>. Here’s how to use it:</p>
<pre data-type="programlisting">public class PCQueue : IDisposable
{
  <strong>BlockingCollection&lt;Action&gt; _taskQ = new BlockingCollection&lt;Action&gt;();</strong>

  public PCQueue (int workerCount)
  {
    // Create and start a separate Task for each consumer:
    for (int i = 0; i &lt; workerCount; i++)
      Task.Factory.StartNew (Consume);
  }

  public void Enqueue (Action action) { <strong>_taskQ.Add (action);</strong> }

  void Consume()
  {
    // This sequence that we’re enumerating will <em>block</em> when no elements
    // are available and will <em>end</em> when CompleteAdding is called.

    foreach (Action action in <strong>_taskQ.GetConsumingEnumerable()</strong>)
      action();     // Perform task.
  }

  public void Dispose() { <strong>_taskQ.CompleteAdding()</strong>; }
}</pre>
<p>Because we didn’t pass anything into <code>BlockingCollection</code>’s constructor, it instantiated a concurrent queue automatically. Had we passed in a <code>ConcurrentStack</code>, we’d have ended up with a producer/consumer stack.</p>
<section data-pdf-bookmark="Using Tasks" data-type="sect3"><div class="sect3" id="using_tasks">
<h3>Using Tasks</h3>
<p><a contenteditable="false" data-primary="BlockingCollection&lt;T&gt;" data-secondary="using tasks" data-type="indexterm" id="ch22.html1033"/>The producer/consumer that we just wrote is inflexible in that we can’t track work items after they’ve been enqueued. It would be nice if we could do the following:</p>
<ul>
<li><p>Know when a work item has completed (and <code>await</code> it)</p></li>
<li><p>Cancel a work item</p></li>
<li><p>Deal elegantly with any exceptions thrown by a work item</p></li>
</ul>
<p>An ideal solution would be to have the <code>Enqueue</code> method return some object giving us the functionality just described. The good news is that a class already exists to do exactly this—the <code>Task</code> class, which we can generate either with a <code>TaskCompletionSource</code> or by instantiating directly (creating an unstarted or <em>cold</em> task):</p>
<pre data-type="programlisting">public class PCQueue : IDisposable
{
  BlockingCollection&lt;Task&gt; _taskQ = new BlockingCollection&lt;Task&gt;();

  public PCQueue (int workerCount)
  {
    // Create and start a separate Task for each consumer:
    for (int i = 0; i &lt; workerCount; i++)
      Task.Factory.StartNew (Consume);
  }

  public Task Enqueue (Action action, CancellationToken cancelToken
                                            = default (CancellationToken))
  {
    var task = new Task (action, cancelToken);
    _taskQ.Add (task);
    return task;
  }

  public Task&lt;TResult&gt; Enqueue&lt;TResult&gt; (Func&lt;TResult&gt; func, 
              CancellationToken cancelToken = default (CancellationToken))
  {
    var task = new Task&lt;TResult&gt; (func, cancelToken);
    _taskQ.Add (task);
    return task;
  }
  
  void Consume()
  {
    foreach (var task in _taskQ.GetConsumingEnumerable())
      try 
      {
          if (!task.IsCanceled) task.RunSynchronously();
      } 
      catch (InvalidOperationException) { }  // Race condition
  }

  public void Dispose() { _taskQ.CompleteAdding(); }
}</pre>
<p>In <code>Enqueue</code>, we enqueue and return to the caller a task that we create but don’t start.</p>
<p>In <code>Consume</code>, we run the task synchronously on the consumer’s thread. We catch an <code>InvalidOperationException</code> to handle the unlikely event that the task is canceled in between checking whether it’s canceled and running it.</p>
<p>Here’s how we can use this class:</p>
<pre data-type="programlisting">var pcQ = new PCQueue (2);    // Maximum concurrency of 2
string result = await pcQ.Enqueue (() =&gt; "That was easy!");
...</pre>
<p>Hence, we have all the benefits of tasks—with exception propagation, return values, and cancellation—while taking complete control over scheduling.<a contenteditable="false" data-primary="" data-startref="ch22.html1033" data-type="indexterm" id="id4560"/><a contenteditable="false" data-primary="" data-startref="ch22.html1032" data-type="indexterm" id="id4561"/><a contenteditable="false" data-primary="" data-startref="ch22.html1031" data-type="indexterm" id="id4562"/><a contenteditable="false" data-primary="" data-startref="ch22.html1030" data-type="indexterm" id="id4563"/><a contenteditable="false" data-primary="" data-startref="ch22.html1029" data-type="indexterm" id="id4564"/><a contenteditable="false" data-primary="" data-startref="ch22.html100" data-type="indexterm" id="id4565"/></p>
</div></section>
</div></section>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="ch12fn1"><sup><a href="ch22.html#ch12fn1-marker">1</a></sup> Due to an implementation detail, there actually needs to be at least two elements to avoid contention entirely.</p></div></div></section></body></html>